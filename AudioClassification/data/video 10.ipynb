{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import glob\n",
    "import moviepy.editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video10\\*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "chunk:   0%|                                                                        | 0/4124 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video10\\Celebrities Read Mean Tweets #11.mp4\n",
      "MoviePy - Writing audio in C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video10\\Celebrities Read Mean Tweets #11.mp4_new_audio_.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in files:\n",
    "    video=moviepy.editor.VideoFileClip(i)\n",
    "    audio=video.audio\n",
    "    print(i)\n",
    "    path= i+\"_\"+'new_audio_.wav'\n",
    "    audio.write_audiofile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = glob.glob(r\"C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video10\\*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\pradeep\\\\text\\\\UPDATE ON 8-DEC-2020\\\\validation\\\\csv file\\\\files\\\\video10\\\\Celebrities Read Mean Tweets #11.mp4_new_audio_.wav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SplitWavAudioMubin():\n",
    "    def __init__(self, folder, filename):\n",
    "        self.folder = folder\n",
    "        self.filename = filename\n",
    "        self.filepath = folder + '\\\\' + filename\n",
    "        \n",
    "        self.audio = AudioSegment.from_wav(self.filepath)\n",
    "    \n",
    "    def get_duration(self):\n",
    "        return self.audio.duration_seconds\n",
    "    \n",
    "    def single_split(self, from_min, to_min, split_filename):\n",
    "        t1 = from_min * 60 * 1000\n",
    "        t2 = to_min * 60 * 1000\n",
    "        split_audio = self.audio[t1:t2]\n",
    "        split_audio.export(self.folder + '\\\\' + split_filename, format=\"wav\")\n",
    "        \n",
    "    def multiple_split(self, min_per_split):\n",
    "        total_mins = math.ceil(self.get_duration() / 60)\n",
    "        for i in range(0, total_mins, min_per_split):\n",
    "            split_fn = str(i) + '_' + self.filename\n",
    "            self.single_split(i, i+min_per_split, split_fn)\n",
    "            print(str(i) + ' Done')\n",
    "            if i == total_mins - min_per_split:\n",
    "                print('All splited successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video10\\Celebrities Read Mean Tweets #11.mp4_new_audio_.wav\n"
     ]
    }
   ],
   "source": [
    "for i in files1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = r'C:\\Users\\PRAVEEN\\Desktop\\Hate Speech\\videos'\n",
    "#for i in files1:\n",
    "    #file=i\n",
    "    #split_wav = SplitWavAudioMubin(folder, i)\n",
    "    #split_wav.multiple_split(min_per_split=1)\n",
    "#file = 'videoplayback (1).mp4_new_audio_.wav'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# create a speech recognition object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition\n",
    "aud1=[]\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "                #aud1.append(whole_text)\n",
    "    aud1.append(whole_text)           \n",
    "    # return the text for all chunks detected\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video10\\Celebrities Read Mean Tweets #11.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Gal gadot. \n",
      "audio-chunks\\chunk2.wav : I am. \n",
      "Error: \n",
      "audio-chunks\\chunk4.wav : I am. \n",
      "audio-chunks\\chunk5.wav : I am am i. \n",
      "audio-chunks\\chunk6.wav : Is there a work i am a b1 range. \n",
      "audio-chunks\\chunk7.wav : Why women cheat. \n",
      "Error: \n",
      "audio-chunks\\chunk9.wav : Emma watson steve cycle type of girl for performance with three days and and cattle is called. \n",
      "audio-chunks\\chunk10.wav : Ke kyon has the most fun to face wash time i like nothing more than to suck him in his agri soft starry eyed frog face. \n",
      "audio-chunks\\chunk11.wav : Badmash looked stunning. \n",
      "audio-chunks\\chunk12.wav : I think she can clean up well despite my grandmother harsh opinion that she is serious. \n",
      "audio-chunks\\chunk13.wav : Adjust the john whiskers ball sack looks exactly like hystrix my face is not matter but my boss is binomo. \n",
      "audio-chunks\\chunk14.wav : Flipkart accepted new barauni show time. \n",
      "audio-chunks\\chunk15.wav : Frowning urdu geography temperature. \n",
      "audio-chunks\\chunk16.wav : Chess.com. \n",
      "audio-chunks\\chunk17.wav : How to stick to steaming for vagina and sharp aniston is what happens when a bag of flour get it big break. \n",
      "audio-chunks\\chunk18.wav : I am a bag of flowers. \n",
      "Error: \n",
      "audio-chunks\\chunk20.wav : Books like the wind through my wisdom be became the wife. \n",
      "audio-chunks\\chunk21.wav : Tryptomer sex offender. \n",
      "audio-chunks\\chunk22.wav : Faisal don. \n",
      "audio-chunks\\chunk23.wav : Simple person ideas for school project with and would never want to hang out with her otherwise. \n",
      "audio-chunks\\chunk24.wav : Enthusiastic. \n",
      "audio-chunks\\chunk25.wav : Harda to ugly whites. \n",
      "audio-chunks\\chunk26.wav : Just tell me i smell like mike orchid nanofire should take 1 min surface. \n",
      "audio-chunks\\chunk27.wav : Price just started watching saturday night live on whatsapp. \n",
      "audio-chunks\\chunk28.wav : Biased not funny. \n",
      "audio-chunks\\chunk29.wav : Personation just any verse. \n",
      "audio-chunks\\chunk30.wav : Sir. \n",
      "audio-chunks\\chunk31.wav : Ishq mein online gianis colours. \n",
      "Error: \n",
      "audio-chunks\\chunk33.wav : Every shade of your mom's lipstick and hurt but if you like that subscribe to our youtube channel for latest videos and its tributaries. \n"
     ]
    }
   ],
   "source": [
    "for i in files1:\n",
    "    print(i)\n",
    "    get_large_audio_transcription(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "aud1=pd.DataFrame(aud1)\n",
    "aud1.replace(\"[^a-zA-Z.]\",\" \",regex=True,inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "aud1.replace(\"\", nan_value, inplace=True)\n",
    "aud1.dropna(axis=0,how='any',thresh=None,subset=None,inplace=True)\n",
    "data=aud1\n",
    "#len(data)\n",
    "headline=[]\n",
    "for row in range(0,len(data.index)):\n",
    "    headline.append(''.join(str(x) for x in data.iloc[row]))\n",
    "headline=str(headline)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline.lower()\n",
    "sentence=headline.split(\".\")\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "tokenText = []\n",
    "for sent in sentence:\n",
    "    tok = nltk.word_tokenize(sent)\n",
    "    if len(tok) > 0:\n",
    "        tokenText.append(tok)      \n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_sentence_new = [] \n",
    "filtered_sentence_new = [word for word in tokenText if not word in stopwords.words()]\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def wordlema(text):\n",
    "    lem_text=\" \".join([lemmatizer.lemmatize(i) for i in  text])\n",
    "    return lem_text\n",
    "lemtxt1 = []\n",
    "for tok in filtered_sentence_new:\n",
    "    lemSent = wordlema(tok)\n",
    "    lemtxt1.append(lemSent)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud2=pd.DataFrame(lemtxt1)\n",
    "aud2.replace(\"[^a-zA-Z.]\",\" \",regex=True,inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "aud2.replace(\"\", nan_value, inplace=True)\n",
    "aud2.dropna(axis=0,how='any',thresh=None,subset=None,inplace=True)\n",
    "data2=aud2\n",
    "#len(data)\n",
    "headline_new=[]\n",
    "for row in range(0,len(data2.index)):\n",
    "    headline_new.append(''.join(str(x) for x in data2.iloc[row]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   Gal gadot',\n",
       " 'I am',\n",
       " 'I am',\n",
       " 'I am am i',\n",
       " 'Is there a work i am a b range',\n",
       " 'Why woman cheat',\n",
       " 'Emma watson steve cycle type of girl for performance with three day and and cattle is called',\n",
       " 'Ke kyon ha the most fun to face wash time i like nothing more than to suck him in his agri soft starry eyed frog face',\n",
       " 'Badmash looked stunning',\n",
       " 'I think she can clean up well despite my grandmother harsh opinion that she is serious',\n",
       " 'Adjust the john whisker ball sack look exactly like hystrix my face is not matter but my bos is binomo',\n",
       " 'Flipkart accepted new barauni show time',\n",
       " 'Frowning urdu geography temperature',\n",
       " 'Chess',\n",
       " 'com',\n",
       " 'How to stick to steaming for vagina and sharp aniston is what happens when a bag of flour get it big break',\n",
       " 'I am a bag of flower',\n",
       " 'Books like the wind through my wisdom be became the wife',\n",
       " 'Tryptomer sex offender',\n",
       " 'Faisal don',\n",
       " 'Simple person idea for school project with and would never want to hang out with her otherwise',\n",
       " 'Enthusiastic',\n",
       " 'Harda to ugly white',\n",
       " 'Just tell me i smell like mike orchid nanofire should take min surface',\n",
       " 'Price just started watching saturday night live on whatsapp',\n",
       " 'Biased not funny',\n",
       " 'Personation just any verse',\n",
       " 'Sir',\n",
       " 'Ishq mein online gianis colour',\n",
       " 'Every shade of your mom s lipstick and hurt but if you like that subscribe to our youtube channel for latest video and it tributary',\n",
       " '   ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline=pd.DataFrame(headline_new)\n",
    "headline.to_csv('video_10.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model=gensim.models.Word2Vec(headline_new,workers=3,size=100,min_count=40,window=10,sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words):\n",
    "  #for sent in list_of_sent:\n",
    "  feature_vec=np.zeros((100,),dtype='float32')\n",
    "  nwords = 0\n",
    "  index2word_set= set(w2v_model.wv.index2word) \n",
    "  for word in words:\n",
    "    if word in index2word_set:\n",
    "        nwords +=1\n",
    "        feature_vec=np.add(feature_vec,w2v_model[word])     \n",
    "  feature_vec=np.divide(feature_vec,nwords)\n",
    "  feature_vec = np.around(feature_vec,3)\n",
    "  return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_feature_vec(tweets):\n",
    "    c=0\n",
    "    tweet_feature_vec = np.zeros((len(tweets),100),dtype='float32')\n",
    "    for tweet in tweets:\n",
    "      tweet_feature_vec[c]=make_feature_vec(tweet)\n",
    "      c=c+1\n",
    "    return tweet_feature_vec\n",
    "#tweets=[]\n",
    "#for tweet in df['tweet']:\n",
    "  #tweets.append(tweet)\n",
    "x1= get_avg_feature_vec(headline_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1=pd.DataFrame(x1)\n",
    "x1.fillna(x1.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "gboost_from_joblib = joblib.load(r'C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\word2vecsamp.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "gboost_from_joblib.predict(x1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 546)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming Text Using TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3),max_features=10000)\n",
    "tfidf.fit(headline_new)\n",
    "headline_new1= tfidf.transform(headline_new)\n",
    "headline_new1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'hate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_from_joblib = joblib.load(r'C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\tfidfsamp546.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "svm_from_joblib.predict(headline_new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "window_size = 40\n",
    "min_word = 5\n",
    "down_sampling = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(headline_new,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words):\n",
    "  #for sent in list_of_sent:\n",
    "  feature_vec=np.zeros((100,),dtype='float32')\n",
    "  nwords = 0\n",
    "  index2word_set= set(ft_model.wv.index2word) \n",
    "  for word in words:\n",
    "    if word in index2word_set:\n",
    "        nwords +=1\n",
    "        feature_vec=np.add(feature_vec,ft_model[word])     \n",
    "  feature_vec=np.divide(feature_vec,nwords)\n",
    "  feature_vec = np.around(feature_vec,3)\n",
    "  return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_feature_vec(tweets):\n",
    "    c=0\n",
    "    tweet_feature_vec = np.zeros((len(tweets),100),dtype='float32')\n",
    "    for tweet in tweets:\n",
    "      tweet_feature_vec[c]=make_feature_vec(tweet)\n",
    "      c=c+1\n",
    "    return tweet_feature_vec\n",
    "#tweets=[]\n",
    "#for tweet in df['tweet']:\n",
    "  #tweets.append(tweet)\n",
    "x= get_avg_feature_vec(headline_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientboost_from_joblib = joblib.load(r'C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\fasttextsamp.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "gradientboost_from_joblib.predict(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=9, n_estimators=300, random_state=10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientboost_from_joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Gal gadot</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I am</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I am</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>I am am i</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Is there a work i am a b range</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Why woman cheat</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Emma watson steve cycle type of girl for perfo...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Ke kyon ha the most fun to face wash time i li...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Badmash looked stunning</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>I think she can clean up well despite my grand...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Adjust the john whisker ball sack look exactly...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>Flipkart accepted new barauni show time</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Frowning urdu geography temperature</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Chess</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>com</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>How to stick to steaming for vagina and sharp ...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>I am a bag of flower</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Books like the wind through my wisdom be becam...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>Tryptomer sex offender</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>Faisal don</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Simple person idea for school project with and...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>Enthusiastic</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>Harda to ugly white</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>Just tell me i smell like mike orchid nanofire...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>Price just started watching saturday night liv...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>Biased not funny</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>Personation just any verse</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Sir</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>Ishq mein online gianis colour</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>Every shade of your mom s lipstick and hurt bu...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                                  0   Label\n",
       "0            0                                          Gal gadot  noHate\n",
       "1            1                                               I am  noHate\n",
       "2            2                                               I am  noHate\n",
       "3            3                                          I am am i  noHate\n",
       "4            4                     Is there a work i am a b range  noHate\n",
       "5            5                                    Why woman cheat  noHate\n",
       "6            6  Emma watson steve cycle type of girl for perfo...  noHate\n",
       "7            7  Ke kyon ha the most fun to face wash time i li...    hate\n",
       "8            8                            Badmash looked stunning  noHate\n",
       "9            9  I think she can clean up well despite my grand...  noHate\n",
       "10          10  Adjust the john whisker ball sack look exactly...  noHate\n",
       "11          11            Flipkart accepted new barauni show time  noHate\n",
       "12          12                Frowning urdu geography temperature  noHate\n",
       "13          13                                              Chess  noHate\n",
       "14          14                                                com  noHate\n",
       "15          15  How to stick to steaming for vagina and sharp ...  noHate\n",
       "16          16                               I am a bag of flower  noHate\n",
       "17          17  Books like the wind through my wisdom be becam...  noHate\n",
       "18          18                             Tryptomer sex offender  noHate\n",
       "19          19                                         Faisal don  noHate\n",
       "20          20  Simple person idea for school project with and...  noHate\n",
       "21          21                                       Enthusiastic  noHate\n",
       "22          22                                Harda to ugly white  noHate\n",
       "23          23  Just tell me i smell like mike orchid nanofire...  noHate\n",
       "24          24  Price just started watching saturday night liv...  noHate\n",
       "25          25                                   Biased not funny  noHate\n",
       "26          26                         Personation just any verse  noHate\n",
       "27          27                                                Sir  noHate\n",
       "28          28                     Ishq mein online gianis colour  noHate\n",
       "29          29  Every shade of your mom s lipstick and hurt bu...  noHate"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline1=pd.read_csv('video_10.csv')\n",
    "headline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline1=headline1.iloc[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Gal gadot</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I am</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I am</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I am am i</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Is there a work i am a b range</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Why woman cheat</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Emma watson steve cycle type of girl for perfo...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Ke kyon ha the most fun to face wash time i li...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Badmash looked stunning</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>I think she can clean up well despite my grand...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Adjust the john whisker ball sack look exactly...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Flipkart accepted new barauni show time</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Frowning urdu geography temperature</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Chess</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>com</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>How to stick to steaming for vagina and sharp ...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>I am a bag of flower</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Books like the wind through my wisdom be becam...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Tryptomer sex offender</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Faisal don</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Simple person idea for school project with and...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Enthusiastic</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Harda to ugly white</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Just tell me i smell like mike orchid nanofire...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Price just started watching saturday night liv...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Biased not funny</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Personation just any verse</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Sir</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Ishq mein online gianis colour</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Every shade of your mom s lipstick and hurt bu...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0   Label\n",
       "0                                           Gal gadot  noHate\n",
       "1                                                I am  noHate\n",
       "2                                                I am  noHate\n",
       "3                                           I am am i  noHate\n",
       "4                      Is there a work i am a b range  noHate\n",
       "5                                     Why woman cheat  noHate\n",
       "6   Emma watson steve cycle type of girl for perfo...  noHate\n",
       "7   Ke kyon ha the most fun to face wash time i li...    hate\n",
       "8                             Badmash looked stunning  noHate\n",
       "9   I think she can clean up well despite my grand...  noHate\n",
       "10  Adjust the john whisker ball sack look exactly...  noHate\n",
       "11            Flipkart accepted new barauni show time  noHate\n",
       "12                Frowning urdu geography temperature  noHate\n",
       "13                                              Chess  noHate\n",
       "14                                                com  noHate\n",
       "15  How to stick to steaming for vagina and sharp ...  noHate\n",
       "16                               I am a bag of flower  noHate\n",
       "17  Books like the wind through my wisdom be becam...  noHate\n",
       "18                             Tryptomer sex offender  noHate\n",
       "19                                         Faisal don  noHate\n",
       "20  Simple person idea for school project with and...  noHate\n",
       "21                                       Enthusiastic  noHate\n",
       "22                                Harda to ugly white  noHate\n",
       "23  Just tell me i smell like mike orchid nanofire...  noHate\n",
       "24  Price just started watching saturday night liv...  noHate\n",
       "25                                   Biased not funny  noHate\n",
       "26                         Personation just any verse  noHate\n",
       "27                                                Sir  noHate\n",
       "28                     Ishq mein online gianis colour  noHate\n",
       "29  Every shade of your mom s lipstick and hurt bu...  noHate"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Label</th>\n",
       "      <th>FastText</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Gal gadot</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I am</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I am</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I am am i</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Is there a work i am a b range</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Why woman cheat</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Emma watson steve cycle type of girl for perfo...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Ke kyon ha the most fun to face wash time i li...</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Badmash looked stunning</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>I think she can clean up well despite my grand...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Adjust the john whisker ball sack look exactly...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Flipkart accepted new barauni show time</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Frowning urdu geography temperature</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Chess</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>com</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>How to stick to steaming for vagina and sharp ...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>I am a bag of flower</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Books like the wind through my wisdom be becam...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Tryptomer sex offender</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Faisal don</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Simple person idea for school project with and...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Enthusiastic</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Harda to ugly white</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Just tell me i smell like mike orchid nanofire...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Price just started watching saturday night liv...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Biased not funny</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Personation just any verse</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Sir</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Ishq mein online gianis colour</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Every shade of your mom s lipstick and hurt bu...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0   Label FastText  \\\n",
       "0                                           Gal gadot  noHate   noHate   \n",
       "1                                                I am  noHate   noHate   \n",
       "2                                                I am  noHate   noHate   \n",
       "3                                           I am am i  noHate   noHate   \n",
       "4                      Is there a work i am a b range  noHate   noHate   \n",
       "5                                     Why woman cheat  noHate   noHate   \n",
       "6   Emma watson steve cycle type of girl for perfo...  noHate   noHate   \n",
       "7   Ke kyon ha the most fun to face wash time i li...    hate   noHate   \n",
       "8                             Badmash looked stunning  noHate   noHate   \n",
       "9   I think she can clean up well despite my grand...  noHate   noHate   \n",
       "10  Adjust the john whisker ball sack look exactly...  noHate   noHate   \n",
       "11            Flipkart accepted new barauni show time  noHate   noHate   \n",
       "12                Frowning urdu geography temperature  noHate   noHate   \n",
       "13                                              Chess  noHate   noHate   \n",
       "14                                                com  noHate   noHate   \n",
       "15  How to stick to steaming for vagina and sharp ...  noHate   noHate   \n",
       "16                               I am a bag of flower  noHate   noHate   \n",
       "17  Books like the wind through my wisdom be becam...  noHate   noHate   \n",
       "18                             Tryptomer sex offender  noHate   noHate   \n",
       "19                                         Faisal don  noHate   noHate   \n",
       "20  Simple person idea for school project with and...  noHate   noHate   \n",
       "21                                       Enthusiastic  noHate   noHate   \n",
       "22                                Harda to ugly white  noHate   noHate   \n",
       "23  Just tell me i smell like mike orchid nanofire...  noHate   noHate   \n",
       "24  Price just started watching saturday night liv...  noHate   noHate   \n",
       "25                                   Biased not funny  noHate   noHate   \n",
       "26                         Personation just any verse  noHate   noHate   \n",
       "27                                                Sir  noHate   noHate   \n",
       "28                     Ishq mein online gianis colour  noHate   noHate   \n",
       "29  Every shade of your mom s lipstick and hurt bu...  noHate   noHate   \n",
       "\n",
       "     TFIDF Word2Vec  \n",
       "0   noHate   noHate  \n",
       "1   noHate   noHate  \n",
       "2   noHate   noHate  \n",
       "3   noHate   noHate  \n",
       "4   noHate   noHate  \n",
       "5   noHate   noHate  \n",
       "6   noHate   noHate  \n",
       "7     hate   noHate  \n",
       "8   noHate   noHate  \n",
       "9   noHate   noHate  \n",
       "10  noHate   noHate  \n",
       "11  noHate   noHate  \n",
       "12  noHate   noHate  \n",
       "13  noHate   noHate  \n",
       "14  noHate   noHate  \n",
       "15  noHate   noHate  \n",
       "16  noHate   noHate  \n",
       "17  noHate   noHate  \n",
       "18  noHate   noHate  \n",
       "19  noHate   noHate  \n",
       "20  noHate   noHate  \n",
       "21  noHate   noHate  \n",
       "22  noHate   noHate  \n",
       "23  noHate   noHate  \n",
       "24  noHate   noHate  \n",
       "25  noHate   noHate  \n",
       "26  noHate   noHate  \n",
       "27  noHate   noHate  \n",
       "28  noHate   noHate  \n",
       "29  noHate   noHate  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline1['FastText']=pd.DataFrame(gradientboost_from_joblib.predict(x))\n",
    "headline1['TFIDF']=pd.DataFrame(svm_from_joblib.predict(headline_new1))\n",
    "headline1['Word2Vec']=pd.DataFrame(gboost_from_joblib.predict(x1))\n",
    "headline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noHate    29\n",
       "hate       1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline1['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1\n",
       "0  0   0\n",
       "1  1  29"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=headline1['Label']\n",
    "pred=headline1['FastText']\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1\n",
       "0  1   0\n",
       "1  0  29"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=headline1['Label']\n",
    "pred1=headline1['TFIDF']\n",
    "pd.DataFrame(confusion_matrix(pred1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1\n",
       "0  0   0\n",
       "1  1  29"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=headline1['Label']\n",
    "pred2=headline1['Word2Vec']\n",
    "pd.DataFrame(confusion_matrix(pred2,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
