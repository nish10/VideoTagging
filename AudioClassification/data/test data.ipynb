{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import glob\n",
    "import moviepy.editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"D:\\vioooo\\trial one\\*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "chunk:   0%|                                                                        | 0/7529 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vioooo\\trial one\\Trump v Biden_ the key moments of the final presidential debate.mp4\n",
      "MoviePy - Writing audio in D:\\vioooo\\trial one\\Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in files:\n",
    "    video=moviepy.editor.VideoFileClip(i)\n",
    "    audio=video.audio\n",
    "    print(i)\n",
    "    path= i+\"_\"+'new_audio_.wav'\n",
    "    audio.write_audiofile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = glob.glob(r\"D:\\vioooo\\trial one\\*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\vioooo\\\\trial one\\\\Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SplitWavAudioMubin():\n",
    "    def __init__(self, folder, filename):\n",
    "        self.folder = folder\n",
    "        self.filename = filename\n",
    "        self.filepath = folder + '\\\\' + filename\n",
    "        \n",
    "        self.audio = AudioSegment.from_wav(self.filepath)\n",
    "    \n",
    "    def get_duration(self):\n",
    "        return self.audio.duration_seconds\n",
    "    \n",
    "    def single_split(self, from_min, to_min, split_filename):\n",
    "        t1 = from_min * 60 * 1000\n",
    "        t2 = to_min * 60 * 1000\n",
    "        split_audio = self.audio[t1:t2]\n",
    "        split_audio.export(self.folder + '\\\\' + split_filename, format=\"wav\")\n",
    "        \n",
    "    def multiple_split(self, min_per_split):\n",
    "        total_mins = math.ceil(self.get_duration() / 60)\n",
    "        for i in range(0, total_mins, min_per_split):\n",
    "            split_fn = str(i) + '_' + self.filename\n",
    "            self.single_split(i, i+min_per_split, split_fn)\n",
    "            print(str(i) + ' Done')\n",
    "            if i == total_mins - min_per_split:\n",
    "                print('All splited successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Done\n",
      "1 Done\n",
      "2 Done\n",
      "3 Done\n",
      "4 Done\n",
      "5 Done\n",
      "All splited successfully\n"
     ]
    }
   ],
   "source": [
    "folder = r'D:\\vioooo\\trial one'\n",
    "file = 'Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav'\n",
    "split_wav = SplitWavAudioMubin(folder, file)\n",
    "split_wav.multiple_split(min_per_split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# create a speech recognition object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition\n",
    "aud1=[]\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "                #aud1.append(whole_text)\n",
    "    aud1.append(whole_text)           \n",
    "    # return the text for all chunks detected\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = glob.glob(r\"D:\\vioooo\\trial one\\*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\vioooo\\\\trial one\\\\0_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\1_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\2_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\3_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\4_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\5_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vioooo\\trial one\\0_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : I have great relationships with other people. \n",
      "audio-chunks\\chunk2.wav : I am released richest person in this world most precious travels you have a modern history. \n",
      "audio-chunks\\chunk3.wav : Post show on every single. \n",
      "audio-chunks\\chunk4.wav : Racist fire. \n",
      "audio-chunks\\chunk5.wav : Donald j trump. \n",
      "audio-chunks\\chunk6.wav : More and more people are. \n",
      "audio-chunks\\chunk7.wav : I getting better we have a problem that a worldwide problem this is a worldwide problem but i've been congratulated by as many countries on what we been able to do it will go away and as i say we've found in the turnover ravan in the cone is going over this is the same for this gun and easter rajasthan is the same for all who told you that even in this by the summer but it got dark winter. \n",
      "D:\\vioooo\\trial one\\1_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Twitter. \n",
      "audio-chunks\\chunk2.wav : He has no clear plan. \n",
      "audio-chunks\\chunk3.wav : Citation can be overwritten. \n",
      "audio-chunks\\chunk4.wav : People learning the diversity. \n",
      "audio-chunks\\chunk5.wav : Describe the black lives matter movement as a symbol of state. \n",
      "audio-chunks\\chunk6.wav : Shared variable and changing white power to millions of your support others you set the clock professional athlete exercising their first amendment rights should be fired usse do americans to say that kind of language is contributing to a climate of hay and ratio of strike i don't know what to say. \n",
      "audio-chunks\\chunk7.wav : I mean i can say anything. \n",
      "audio-chunks\\chunk8.wav : It's very it's mixed me. \n",
      "D:\\vioooo\\trial one\\2_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "Error: \n",
      "audio-chunks\\chunk2.wav : Because i am i am the least. \n",
      "audio-chunks\\chunk3.wav : Racist person i can even see the audience to show the earth. \n",
      "audio-chunks\\chunk4.wav : Ranu mandal list. \n",
      "audio-chunks\\chunk5.wav : Richest person in this world. \n",
      "audio-chunks\\chunk6.wav : Nobody has done more for the black. \n",
      "audio-chunks\\chunk7.wav : Comedi. \n",
      "audio-chunks\\chunk8.wav : With the exception of abraham lincoln except exception of abraham lincoln. \n",
      "audio-chunks\\chunk9.wav : Hippo show on every single. \n",
      "audio-chunks\\chunk10.wav : Racist fire. \n",
      "audio-chunks\\chunk11.wav : Abhishek bhagwan. \n",
      "audio-chunks\\chunk12.wav : It vary with trying very hard for this kit comes with the paracetamol with bottles in through origin. \n",
      "Error: \n",
      "audio-chunks\\chunk14.wav : Separate from the pant. \n",
      "audio-chunks\\chunk15.wav : Excess laughing stock in. \n",
      "audio-chunks\\chunk16.wav : Bible every notion of your. \n",
      "D:\\vioooo\\trial one\\3_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Nation. \n",
      "audio-chunks\\chunk2.wav : When you say the come back they don't come back you say never come back only the really. \n",
      "audio-chunks\\chunk3.wav : I said to say this. \n",
      "audio-chunks\\chunk4.wav : 50 with the lowest iq he might come back. \n",
      "audio-chunks\\chunk5.wav : Is the problem when you run it it's no good so i'd like to terminate obamacare come up with a brand new beautiful very costly american people because disturb coronavirus in economics pullover. \n",
      "audio-chunks\\chunk6.wav : Will you people have lost their private insurance. \n",
      "audio-chunks\\chunk7.wav : Anyone to check kuwait 22 million more people have another barmer care and over 110 billion people praising condition of people from covid-19 have pre-existing condition what are they gonna do. \n",
      "D:\\vioooo\\trial one\\4_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Agar achievers point 1 million on youtube i could find plan europe because the kind of things you've done in the kind of money is that your family is taken when you brother make money in a rani millions of dollars for the brother brother made of water and its three years in the say you get some of it and you deliver value of houses all over the place you live. \n",
      "audio-chunks\\chunk2.wav : Mewati. \n",
      "audio-chunks\\chunk3.wav : Reservation for. \n",
      "audio-chunks\\chunk4.wav : Hidden want to talk about this this issue. \n",
      "audio-chunks\\chunk5.wav : His family. \n",
      "audio-chunks\\chunk6.wav : Your friend. \n",
      "audio-chunks\\chunk7.wav : You keep talking about these things should you do when you get there just a short time ago and you guys did nothing with their job and i really because i read because of barack obama for job. \n",
      "D:\\vioooo\\trial one\\5_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Because i love u i look at you now your politician are run because i love you. \n",
      "audio-chunks\\chunk2.wav : Gopichand who became a because what happened in here is. \n",
      "audio-chunks\\chunk3.wav : You know who i am. \n",
      "audio-chunks\\chunk4.wav : You know who he was. \n",
      "audio-chunks\\chunk5.wav : You know which character you know my cat. \n",
      "audio-chunks\\chunk6.wav : Innova piche syster anarkali natur. \n",
      "audio-chunks\\chunk7.wav : I am anxious to have this raised. \n",
      "audio-chunks\\chunk8.wav : I am anxious to see this take place. \n",
      "audio-chunks\\chunk9.wav : I am. \n",
      "audio-chunks\\chunk10.wav : Character of the countries on the ballot ok address book is costly. \n"
     ]
    }
   ],
   "source": [
    "for i in files1:\n",
    "    print(i)\n",
    "    get_large_audio_transcription(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "aud1=pd.DataFrame(aud1)\n",
    "aud1.replace(\"[^a-zA-Z.]\",\" \",regex=True,inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "aud1.replace(\"\", nan_value, inplace=True)\n",
    "aud1.dropna(axis=0,how='any',thresh=None,subset=None,inplace=True)\n",
    "data=aud1\n",
    "#len(data)\n",
    "headline=[]\n",
    "for row in range(0,len(data.index)):\n",
    "    headline.append(''.join(str(x) for x in data.iloc[row]))\n",
    "headline=str(headline)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline.lower()\n",
    "sentence=headline.split(\".\")\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "tokenText = []\n",
    "for sent in sentence:\n",
    "    tok = nltk.word_tokenize(sent)\n",
    "    if len(tok) > 0:\n",
    "        tokenText.append(tok)      \n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_sentence_new = [] \n",
    "filtered_sentence_new = [word for word in tokenText if not word in stopwords.words()]\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def wordlema(text):\n",
    "    lem_text=\" \".join([lemmatizer.lemmatize(i) for i in  text])\n",
    "    return lem_text\n",
    "lemtxt1 = []\n",
    "for tok in filtered_sentence_new:\n",
    "    lemSent = wordlema(tok)\n",
    "    lemtxt1.append(lemSent)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud2=pd.DataFrame(lemtxt1)\n",
    "aud2.replace(\"[^a-zA-Z.]\",\" \",regex=True,inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "aud2.replace(\"\", nan_value, inplace=True)\n",
    "aud2.dropna(axis=0,how='any',thresh=None,subset=None,inplace=True)\n",
    "data2=aud2\n",
    "#len(data)\n",
    "headline_new=[]\n",
    "for row in range(0,len(data2.index)):\n",
    "    headline_new.append(''.join(str(x) for x in data2.iloc[row]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hatesonar import Sonar\n",
    "sonar=Sonar()\n",
    "aa=[]\n",
    "for i in headline_new:\n",
    "    a=sonar.ping(text=i)\n",
    "    aa.append(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '    I have great relationship with other people',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04862927603487373},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.346704100256847},\n",
       "   {'class_name': 'neither', 'confidence': 0.6046666237082794}]},\n",
       " {'text': 'I am released richest person in this world most precious travel you have a modern history',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.046146943457816635},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.356225234406434},\n",
       "   {'class_name': 'neither', 'confidence': 0.5976278221357494}]},\n",
       " {'text': 'Post show on every single',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.05092347527025326},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.2700083705273619},\n",
       "   {'class_name': 'neither', 'confidence': 0.6790681542023849}]},\n",
       " {'text': 'Racist fire',\n",
       "  'top_class': 'hate_speech',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.6039441725149312},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.05446957480245189},\n",
       "   {'class_name': 'neither', 'confidence': 0.3415862526826169}]},\n",
       " {'text': 'Donald j trump',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'More and more people are',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.11834301046776778},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.22187993659292596},\n",
       "   {'class_name': 'neither', 'confidence': 0.6597770529393062}]},\n",
       " {'text': 'I getting better we have a problem that a worldwide problem this is a worldwide problem but i ve been congratulated by a many country on what we been able to do it will go away and a i say we ve found in the turnover ravan in the cone is going over this is the same for this gun and easter rajasthan is the same for all who told you that even in this by the summer but it got dark winter',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03159463800651686},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.38404836730827047},\n",
       "   {'class_name': 'neither', 'confidence': 0.5843569946852127}]},\n",
       " {'text': '     Twitter',\n",
       "  'top_class': 'offensive_language',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.038406706191623896},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.5923891351514252},\n",
       "   {'class_name': 'neither', 'confidence': 0.3692041586569509}]},\n",
       " {'text': 'He ha no clear plan',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.043209058122855165},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.34052690907200966},\n",
       "   {'class_name': 'neither', 'confidence': 0.6162640328051352}]},\n",
       " {'text': 'Citation can be overwritten',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04129080116744889},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3284317748669212},\n",
       "   {'class_name': 'neither', 'confidence': 0.63027742396563}]},\n",
       " {'text': 'People learning the diversity',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.06246691789236795},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.314747490859658},\n",
       "   {'class_name': 'neither', 'confidence': 0.6227855912479741}]},\n",
       " {'text': 'Describe the black life matter movement a a symbol of state',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.0731896304809014},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.27900115984877055},\n",
       "   {'class_name': 'neither', 'confidence': 0.6478092096703281}]},\n",
       " {'text': 'Shared variable and changing white power to million of your support others you set the clock professional athlete exercising their first amendment right should be fired usse do american to say that kind of language is contributing to a climate of hay and ratio of strike i don t know what to say',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.191504862405811},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3659915015762295},\n",
       "   {'class_name': 'neither', 'confidence': 0.44250363601795945}]},\n",
       " {'text': 'I mean i can say anything',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03977251737065871},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.32495658222083595},\n",
       "   {'class_name': 'neither', 'confidence': 0.6352709004085054}]},\n",
       " {'text': 'It s very it s mixed me',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.0306982940878198},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3666546396727711},\n",
       "   {'class_name': 'neither', 'confidence': 0.602647066239409}]},\n",
       " {'text': '     Because i am i am the least',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03836156592677474},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.31659180017084537},\n",
       "   {'class_name': 'neither', 'confidence': 0.6450466339023799}]},\n",
       " {'text': 'Racist person i can even see the audience to show the earth',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.3450101058343368},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.10943300472509733},\n",
       "   {'class_name': 'neither', 'confidence': 0.5455568894405659}]},\n",
       " {'text': 'Ranu mandal list',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'Richest person in this world',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.0393730053503985},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3317373438479406},\n",
       "   {'class_name': 'neither', 'confidence': 0.6288896508016609}]},\n",
       " {'text': 'Nobody ha done more for the black',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04290019898008495},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.4008532918487348},\n",
       "   {'class_name': 'neither', 'confidence': 0.5562465091711801}]},\n",
       " {'text': 'Comedi',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'With the exception of abraham lincoln except exception of abraham lincoln',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03519218683287281},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33748425115527825},\n",
       "   {'class_name': 'neither', 'confidence': 0.6273235620118489}]},\n",
       " {'text': 'Hippo show on every single',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.0499419314199617},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.2747089110901462},\n",
       "   {'class_name': 'neither', 'confidence': 0.6753491574898922}]},\n",
       " {'text': 'Racist fire',\n",
       "  'top_class': 'hate_speech',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.6039441725149312},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.05446957480245189},\n",
       "   {'class_name': 'neither', 'confidence': 0.3415862526826169}]},\n",
       " {'text': 'Abhishek bhagwan',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'It vary with trying very hard for this kit come with the paracetamol with bottle in through origin',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.019341062896657074},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.474080012278628},\n",
       "   {'class_name': 'neither', 'confidence': 0.506578924824715}]},\n",
       " {'text': 'Separate from the pant',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03955996402307523},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3019722825957322},\n",
       "   {'class_name': 'neither', 'confidence': 0.6584677533811926}]},\n",
       " {'text': 'Excess laughing stock in',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'Bible every notion of your',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.06378393371524331},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.32362742979535686},\n",
       "   {'class_name': 'neither', 'confidence': 0.6125886364894}]},\n",
       " {'text': '     Nation',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'When you say the come back they don t come back you say never come back only the really',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04253583603183492},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.4312383183839973},\n",
       "   {'class_name': 'neither', 'confidence': 0.5262258455841679}]},\n",
       " {'text': 'I said to say this',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04983299158041454},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.326679471464465},\n",
       "   {'class_name': 'neither', 'confidence': 0.6234875369551205}]},\n",
       " {'text': 'with the lowest iq he might come back',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03346429067752405},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.36119509407257855},\n",
       "   {'class_name': 'neither', 'confidence': 0.6053406152498972}]},\n",
       " {'text': 'Is the problem when you run it it s no good so i d like to terminate obamacare come up with a brand new beautiful very costly american people because disturb coronavirus in economics pullover',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.05883643688788579},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3806063317574244},\n",
       "   {'class_name': 'neither', 'confidence': 0.5605572313546899}]},\n",
       " {'text': 'Will you people have lost their private insurance',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.07853630655497419},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3499320230388019},\n",
       "   {'class_name': 'neither', 'confidence': 0.5715316704062239}]},\n",
       " {'text': 'Anyone to check kuwait million more people have another barmer care and over billion people praising condition of people from covid have pre existing condition what are they gon na do',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.08487830020800742},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3405700099450947},\n",
       "   {'class_name': 'neither', 'confidence': 0.574551689846898}]},\n",
       " {'text': '     Agar achiever point million on youtube i could find plan europe because the kind of thing you ve done in the kind of money is that your family is taken when you brother make money in a rani million of dollar for the brother brother made of water and it three year in the say you get some of it and you deliver value of house all over the place you live',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.051271653174658406},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33947538386686743},\n",
       "   {'class_name': 'neither', 'confidence': 0.6092529629584742}]},\n",
       " {'text': 'Mewati',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'Reservation for',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.023896577328981292},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.2538532852897668},\n",
       "   {'class_name': 'neither', 'confidence': 0.7222501373812519}]},\n",
       " {'text': 'Hidden want to talk about this this issue',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.02946221727410455},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.35115126625951815},\n",
       "   {'class_name': 'neither', 'confidence': 0.6193865164663773}]},\n",
       " {'text': 'His family',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.08363131541617598},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3315292544294772},\n",
       "   {'class_name': 'neither', 'confidence': 0.5848394301543467}]},\n",
       " {'text': 'Your friend',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.10222989621622083},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.31275809515180064},\n",
       "   {'class_name': 'neither', 'confidence': 0.5850120086319786}]},\n",
       " {'text': 'You keep talking about these thing should you do when you get there just a short time ago and you guy did nothing with their job and i really because i read because of barack obama for job',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.05452184031057339},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.46226894490711334},\n",
       "   {'class_name': 'neither', 'confidence': 0.4832092147823132}]},\n",
       " {'text': '     Because i love u i look at you now your politician are run because i love you',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.11979551076816354},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3890496007112006},\n",
       "   {'class_name': 'neither', 'confidence': 0.49115488852063593}]},\n",
       " {'text': 'Gopichand who became a because what happened in here is',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.09445769942389108},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3409997131088192},\n",
       "   {'class_name': 'neither', 'confidence': 0.5645425874672897}]},\n",
       " {'text': 'You know who i am',\n",
       "  'top_class': 'offensive_language',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.06820745847746104},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.5198113117672538},\n",
       "   {'class_name': 'neither', 'confidence': 0.4119812297552853}]},\n",
       " {'text': 'You know who he wa',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.06640388579085858},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.4543161835749855},\n",
       "   {'class_name': 'neither', 'confidence': 0.4792799306341559}]},\n",
       " {'text': 'You know which character you know my cat',\n",
       "  'top_class': 'offensive_language',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.0538001601377588},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.4781797571909101},\n",
       "   {'class_name': 'neither', 'confidence': 0.468020082671331}]},\n",
       " {'text': 'Innova piche syster anarkali natur',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'I am anxious to have this raised',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04411997174969791},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33575011660330056},\n",
       "   {'class_name': 'neither', 'confidence': 0.6201299116470016}]},\n",
       " {'text': 'I am anxious to see this take place',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.0471650825108245},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.28716625154607606},\n",
       "   {'class_name': 'neither', 'confidence': 0.6656686659430995}]},\n",
       " {'text': 'I am',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03902346103501681},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3305238979082658},\n",
       "   {'class_name': 'neither', 'confidence': 0.6304526410567174}]},\n",
       " {'text': 'Character of the country on the ballot ok address book is costly',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.044024078140000374},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3187482869435884},\n",
       "   {'class_name': 'neither', 'confidence': 0.6372276349164111}]},\n",
       " {'text': '   ',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnew=pd.DataFrame(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>top_class</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I have great relationship with other people</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I am released richest person in this world mos...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Post show on every single</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Racist fire</td>\n",
       "      <td>hate_speech</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Donald j trump</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>More and more people are</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>I getting better we have a problem that a worl...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Twitter</td>\n",
       "      <td>offensive_language</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>He ha no clear plan</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Citation can be overwritten</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>People learning the diversity</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Describe the black life matter movement a a sy...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Shared variable and changing white power to mi...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>I mean i can say anything</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>It s very it s mixed me</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Because i am i am the least</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Racist person i can even see the audience to s...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Ranu mandal list</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Richest person in this world</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Nobody ha done more for the black</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Comedi</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>With the exception of abraham lincoln except e...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Hippo show on every single</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Racist fire</td>\n",
       "      <td>hate_speech</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Abhishek bhagwan</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>It vary with trying very hard for this kit com...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Separate from the pant</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Excess laughing stock in</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>Bible every notion of your</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>Nation</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>When you say the come back they don t come bac...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>I said to say this</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>with the lowest iq he might come back</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>Is the problem when you run it it s no good so...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>Will you people have lost their private insurance</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>Anyone to check kuwait million more people hav...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>Agar achiever point million on youtube i ...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>Mewati</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>Reservation for</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>Hidden want to talk about this this issue</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>His family</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>Your friend</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>You keep talking about these thing should you ...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>Because i love u i look at you now your p...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>Gopichand who became a because what happened i...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>You know who i am</td>\n",
       "      <td>offensive_language</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>You know who he wa</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>You know which character you know my cat</td>\n",
       "      <td>offensive_language</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>Innova piche syster anarkali natur</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>I am anxious to have this raised</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>I am anxious to see this take place</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>I am</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>Character of the country on the ballot ok addr...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td></td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text           top_class  \\\n",
       "0         I have great relationship with other people             neither   \n",
       "1   I am released richest person in this world mos...             neither   \n",
       "2                           Post show on every single             neither   \n",
       "3                                         Racist fire         hate_speech   \n",
       "4                                      Donald j trump             neither   \n",
       "5                            More and more people are             neither   \n",
       "6   I getting better we have a problem that a worl...             neither   \n",
       "7                                             Twitter  offensive_language   \n",
       "8                                 He ha no clear plan             neither   \n",
       "9                         Citation can be overwritten             neither   \n",
       "10                      People learning the diversity             neither   \n",
       "11  Describe the black life matter movement a a sy...             neither   \n",
       "12  Shared variable and changing white power to mi...             neither   \n",
       "13                          I mean i can say anything             neither   \n",
       "14                            It s very it s mixed me             neither   \n",
       "15                        Because i am i am the least             neither   \n",
       "16  Racist person i can even see the audience to s...             neither   \n",
       "17                                   Ranu mandal list             neither   \n",
       "18                       Richest person in this world             neither   \n",
       "19                  Nobody ha done more for the black             neither   \n",
       "20                                             Comedi             neither   \n",
       "21  With the exception of abraham lincoln except e...             neither   \n",
       "22                         Hippo show on every single             neither   \n",
       "23                                        Racist fire         hate_speech   \n",
       "24                                   Abhishek bhagwan             neither   \n",
       "25  It vary with trying very hard for this kit com...             neither   \n",
       "26                             Separate from the pant             neither   \n",
       "27                           Excess laughing stock in             neither   \n",
       "28                         Bible every notion of your             neither   \n",
       "29                                             Nation             neither   \n",
       "30  When you say the come back they don t come bac...             neither   \n",
       "31                                 I said to say this             neither   \n",
       "32              with the lowest iq he might come back             neither   \n",
       "33  Is the problem when you run it it s no good so...             neither   \n",
       "34  Will you people have lost their private insurance             neither   \n",
       "35  Anyone to check kuwait million more people hav...             neither   \n",
       "36       Agar achiever point million on youtube i ...             neither   \n",
       "37                                             Mewati             neither   \n",
       "38                                    Reservation for             neither   \n",
       "39          Hidden want to talk about this this issue             neither   \n",
       "40                                         His family             neither   \n",
       "41                                        Your friend             neither   \n",
       "42  You keep talking about these thing should you ...             neither   \n",
       "43       Because i love u i look at you now your p...             neither   \n",
       "44  Gopichand who became a because what happened i...             neither   \n",
       "45                                  You know who i am  offensive_language   \n",
       "46                                 You know who he wa             neither   \n",
       "47           You know which character you know my cat  offensive_language   \n",
       "48                 Innova piche syster anarkali natur             neither   \n",
       "49                   I am anxious to have this raised             neither   \n",
       "50                I am anxious to see this take place             neither   \n",
       "51                                               I am             neither   \n",
       "52  Character of the country on the ballot ok addr...             neither   \n",
       "53                                                                neither   \n",
       "\n",
       "                                              classes  \n",
       "0   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "1   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "2   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "3   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "4   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "5   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "6   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "7   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "8   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "9   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "10  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "11  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "12  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "13  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "14  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "15  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "16  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "17  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "18  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "19  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "20  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "21  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "22  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "23  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "24  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "25  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "26  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "27  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "28  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "29  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "30  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "31  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "32  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "33  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "34  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "35  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "36  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "37  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "38  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "39  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "40  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "41  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "42  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "43  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "44  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "45  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "46  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "47  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "48  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "49  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "50  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "51  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "52  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "53  [{'class_name': 'hate_speech', 'confidence': 0...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnew.to_csv(r\"D:\\vioooo\\trial one\\hatesonar result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=cnew['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import csv\n",
    "import docx2txt\n",
    "import numpy\n",
    "# used for looping through folders/files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#Calc tfidf and cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_score(trainDataVecs):\n",
    "    d=[]\n",
    "    dd=[]\n",
    "    for i in range(0,len(trainDataVecs)):\n",
    "        for j in range(i+1,len(trainDataVecs)):\n",
    "            a=cosine_similarity([trainDataVecs[i]], [trainDataVecs[j]])\n",
    "            #anew=max(a)\n",
    "            d.append(a)\n",
    "    return d\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed) 0 as per num_features, in our case 300\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)  # words known to the model which we added here 10500 around \n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            feature_vec = np.add(feature_vec,model[word]) # Calculate the weights if got word \n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords) # divide the weights according to  words  by divinding them number of words in sentence\n",
    "    return feature_vec\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors f      b  or all reviews\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return review_feature_vecs    \n",
    "def max_similarity1(tfs,fileNames):\n",
    "    maxi=0\n",
    "    ids=1\n",
    "    for i in range(0,44):\n",
    "        for j in range(i+1,44):\n",
    "            a=cosine_similarity([trainDataVecs[i]], [trainDataVecs[j]])\n",
    "            a=a[0][0]\n",
    "            #print(str(i+1)+ \" \"+str(a))\n",
    "        #print(value + \",\" + i)\n",
    "            if a>maxi:\n",
    "                ids=i\n",
    "                maxi=a\n",
    "    return [ids+1,maxi]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import docx2txt\n",
    "import glob\n",
    "from langdetect import detect\n",
    "from translate import Translator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import os\n",
    "model_name = 'train_model'\n",
    "# Set values for various word2vec parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 3       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "if not os.path.exists(model_name): \n",
    "    # Initialize and train the model (this will take some time)\n",
    "    model = Word2Vec(filtered_sentence, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(model_name)\n",
    "    \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs = get_avg_feature_vecs(vect, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0097797 , -0.1535192 , -0.08648782, ..., -0.04555966,\n",
       "        -0.0182653 , -0.00650879],\n",
       "       [-0.00670387, -0.1863848 , -0.10041971, ..., -0.01892955,\n",
       "        -0.00283211,  0.01212567],\n",
       "       [-0.01473476, -0.13760836, -0.07964463, ..., -0.08796848,\n",
       "        -0.03973971, -0.03026342],\n",
       "       ...,\n",
       "       [-0.01010666, -0.14651187, -0.08357932, ..., -0.06159288,\n",
       "        -0.02659735, -0.01571397],\n",
       "       [-0.0070706 , -0.14404333, -0.07985536, ..., -0.07315195,\n",
       "        -0.0325175 , -0.02268166],\n",
       "       [ 0.00500207, -0.04175455, -0.00168312, ...,  0.00639704,\n",
       "         0.00326777,  0.01193533]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1=pd.DataFrame(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1=trainDataVecs1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1.to_csv(\"trainDataVecs1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.009780</td>\n",
       "      <td>-0.153519</td>\n",
       "      <td>-0.086488</td>\n",
       "      <td>0.062050</td>\n",
       "      <td>0.059775</td>\n",
       "      <td>0.022163</td>\n",
       "      <td>-0.010749</td>\n",
       "      <td>0.089941</td>\n",
       "      <td>-0.053717</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>-0.026404</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.013745</td>\n",
       "      <td>-0.009221</td>\n",
       "      <td>-0.013890</td>\n",
       "      <td>-0.045560</td>\n",
       "      <td>-0.018265</td>\n",
       "      <td>-0.006509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>-0.186385</td>\n",
       "      <td>-0.100420</td>\n",
       "      <td>0.074612</td>\n",
       "      <td>0.069428</td>\n",
       "      <td>0.019153</td>\n",
       "      <td>-0.011190</td>\n",
       "      <td>0.112470</td>\n",
       "      <td>-0.060208</td>\n",
       "      <td>0.026991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.021816</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>0.033347</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>-0.018930</td>\n",
       "      <td>-0.002832</td>\n",
       "      <td>0.012126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.137608</td>\n",
       "      <td>-0.079645</td>\n",
       "      <td>0.054923</td>\n",
       "      <td>0.049929</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.072751</td>\n",
       "      <td>-0.046585</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013403</td>\n",
       "      <td>-0.042277</td>\n",
       "      <td>0.038408</td>\n",
       "      <td>0.063280</td>\n",
       "      <td>0.033056</td>\n",
       "      <td>-0.023695</td>\n",
       "      <td>-0.052760</td>\n",
       "      <td>-0.087968</td>\n",
       "      <td>-0.039740</td>\n",
       "      <td>-0.030263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.005637</td>\n",
       "      <td>-0.179852</td>\n",
       "      <td>-0.097126</td>\n",
       "      <td>0.074498</td>\n",
       "      <td>0.067334</td>\n",
       "      <td>0.021855</td>\n",
       "      <td>-0.013841</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>-0.059853</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>-0.028097</td>\n",
       "      <td>-0.001646</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>-0.008279</td>\n",
       "      <td>-0.008072</td>\n",
       "      <td>-0.039719</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>-0.002070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.004995</td>\n",
       "      <td>-0.159314</td>\n",
       "      <td>-0.082151</td>\n",
       "      <td>0.064529</td>\n",
       "      <td>0.063772</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>-0.017254</td>\n",
       "      <td>0.103314</td>\n",
       "      <td>-0.054453</td>\n",
       "      <td>0.025985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>-0.009858</td>\n",
       "      <td>-0.028317</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>-0.006525</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.005177</td>\n",
       "      <td>0.019327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>-0.006016</td>\n",
       "      <td>-0.159820</td>\n",
       "      <td>-0.087673</td>\n",
       "      <td>0.063355</td>\n",
       "      <td>0.061493</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>-0.009578</td>\n",
       "      <td>0.093405</td>\n",
       "      <td>-0.052412</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.027015</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>-0.007556</td>\n",
       "      <td>-0.008190</td>\n",
       "      <td>-0.039200</td>\n",
       "      <td>-0.014289</td>\n",
       "      <td>-0.001972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7021</td>\n",
       "      <td>-0.007193</td>\n",
       "      <td>-0.155850</td>\n",
       "      <td>-0.086701</td>\n",
       "      <td>0.061205</td>\n",
       "      <td>0.059566</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>-0.008239</td>\n",
       "      <td>0.092551</td>\n",
       "      <td>-0.052765</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>-0.027081</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.041899</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>-0.008080</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.039849</td>\n",
       "      <td>-0.013918</td>\n",
       "      <td>-0.001909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7022</td>\n",
       "      <td>-0.010107</td>\n",
       "      <td>-0.146512</td>\n",
       "      <td>-0.083579</td>\n",
       "      <td>0.058978</td>\n",
       "      <td>0.057682</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>-0.049590</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011750</td>\n",
       "      <td>-0.032732</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>0.048995</td>\n",
       "      <td>0.020899</td>\n",
       "      <td>-0.014102</td>\n",
       "      <td>-0.027992</td>\n",
       "      <td>-0.061593</td>\n",
       "      <td>-0.026597</td>\n",
       "      <td>-0.015714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7023</td>\n",
       "      <td>-0.007071</td>\n",
       "      <td>-0.144043</td>\n",
       "      <td>-0.079855</td>\n",
       "      <td>0.057958</td>\n",
       "      <td>0.055704</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>-0.047519</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016278</td>\n",
       "      <td>-0.038178</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.052364</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>-0.019552</td>\n",
       "      <td>-0.038423</td>\n",
       "      <td>-0.073152</td>\n",
       "      <td>-0.032518</td>\n",
       "      <td>-0.022682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7024</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>-0.041755</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.031302</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>-0.016998</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032765</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.019534</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>-0.003051</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.011935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7025 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.009780 -0.153519 -0.086488  0.062050  0.059775  0.022163 -0.010749   \n",
       "1    -0.006704 -0.186385 -0.100420  0.074612  0.069428  0.019153 -0.011190   \n",
       "2    -0.014735 -0.137608 -0.079645  0.054923  0.049929  0.020018 -0.007845   \n",
       "3    -0.005637 -0.179852 -0.097126  0.074498  0.067334  0.021855 -0.013841   \n",
       "4    -0.004995 -0.159314 -0.082151  0.064529  0.063772  0.020675 -0.017254   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7020 -0.006016 -0.159820 -0.087673  0.063355  0.061493  0.019642 -0.009578   \n",
       "7021 -0.007193 -0.155850 -0.086701  0.061205  0.059566  0.018958 -0.008239   \n",
       "7022 -0.010107 -0.146512 -0.083579  0.058978  0.057682  0.021916 -0.009674   \n",
       "7023 -0.007071 -0.144043 -0.079855  0.057958  0.055704  0.021280 -0.011102   \n",
       "7024  0.005002 -0.041755 -0.001683  0.006120  0.031302  0.011992 -0.016998   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0     0.089941 -0.053717  0.021990  ... -0.007670 -0.026404  0.004354   \n",
       "1     0.112470 -0.060208  0.026991  ...  0.000254 -0.021816 -0.016526   \n",
       "2     0.072751 -0.046585  0.016632  ... -0.013403 -0.042277  0.038408   \n",
       "3     0.101287 -0.059853  0.024283  ... -0.006704 -0.028097 -0.001646   \n",
       "4     0.103314 -0.054453  0.025985  ... -0.004544 -0.009858 -0.028317   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7020  0.093405 -0.052412  0.023197  ... -0.006700 -0.027015 -0.000707   \n",
       "7021  0.092551 -0.052765  0.022502  ... -0.004267 -0.027081  0.001552   \n",
       "7022  0.084256 -0.049590  0.021110  ... -0.011750 -0.032732  0.015828   \n",
       "7023  0.075940 -0.047519  0.019985  ... -0.016278 -0.038178  0.023187   \n",
       "7024  0.058780 -0.000466  0.012808  ... -0.032765 -0.002467 -0.019534   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0     0.041168  0.013745 -0.009221 -0.013890 -0.045560 -0.018265 -0.006509  \n",
       "1     0.033347 -0.001565  0.000954  0.009685 -0.018930 -0.002832  0.012126  \n",
       "2     0.063280  0.033056 -0.023695 -0.052760 -0.087968 -0.039740 -0.030263  \n",
       "3     0.039481  0.007988 -0.008279 -0.008072 -0.039719 -0.015118 -0.002070  \n",
       "4     0.015599 -0.006525  0.005752  0.022966  0.001285  0.005177  0.019327  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7020  0.040323  0.009394 -0.007556 -0.008190 -0.039200 -0.014289 -0.001972  \n",
       "7021  0.041899  0.010162 -0.008080 -0.009752 -0.039849 -0.013918 -0.001909  \n",
       "7022  0.048995  0.020899 -0.014102 -0.027992 -0.061593 -0.026597 -0.015714  \n",
       "7023  0.052364  0.025306 -0.019552 -0.038423 -0.073152 -0.032518 -0.022682  \n",
       "7024  0.006022  0.003704 -0.003051  0.022616  0.006397  0.003268  0.011935  \n",
       "\n",
       "[7025 rows x 300 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDataVecs1=pd.read_csv(\"trainDataVecs1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame()\n",
    "result=pd.concat([trainDataVecs1,cnew['top_class']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"finaloutput.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result=pd.read_csv(\"finaloutput.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=result.iloc[:,0:299]\n",
    "y=pd.DataFrame(result['top_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42,kind='regular',k_neighbors=2)\n",
    "#X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y=label_encoder.fit_transform(y) \n",
    "#X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.009780</td>\n",
       "      <td>-0.153519</td>\n",
       "      <td>-0.086488</td>\n",
       "      <td>0.062050</td>\n",
       "      <td>0.059775</td>\n",
       "      <td>0.022163</td>\n",
       "      <td>-0.010749</td>\n",
       "      <td>0.089941</td>\n",
       "      <td>-0.053717</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017769</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>-0.026404</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.013745</td>\n",
       "      <td>-0.009221</td>\n",
       "      <td>-0.013890</td>\n",
       "      <td>-0.045560</td>\n",
       "      <td>-0.018265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>-0.186385</td>\n",
       "      <td>-0.100420</td>\n",
       "      <td>0.074612</td>\n",
       "      <td>0.069428</td>\n",
       "      <td>0.019153</td>\n",
       "      <td>-0.011190</td>\n",
       "      <td>0.112470</td>\n",
       "      <td>-0.060208</td>\n",
       "      <td>0.026991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013348</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.021816</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>0.033347</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>-0.018930</td>\n",
       "      <td>-0.002832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.137608</td>\n",
       "      <td>-0.079645</td>\n",
       "      <td>0.054923</td>\n",
       "      <td>0.049929</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.072751</td>\n",
       "      <td>-0.046585</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.013403</td>\n",
       "      <td>-0.042277</td>\n",
       "      <td>0.038408</td>\n",
       "      <td>0.063280</td>\n",
       "      <td>0.033056</td>\n",
       "      <td>-0.023695</td>\n",
       "      <td>-0.052760</td>\n",
       "      <td>-0.087968</td>\n",
       "      <td>-0.039740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.005637</td>\n",
       "      <td>-0.179852</td>\n",
       "      <td>-0.097126</td>\n",
       "      <td>0.074498</td>\n",
       "      <td>0.067334</td>\n",
       "      <td>0.021855</td>\n",
       "      <td>-0.013841</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>-0.059853</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015320</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>-0.028097</td>\n",
       "      <td>-0.001646</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>-0.008279</td>\n",
       "      <td>-0.008072</td>\n",
       "      <td>-0.039719</td>\n",
       "      <td>-0.015118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.004995</td>\n",
       "      <td>-0.159314</td>\n",
       "      <td>-0.082151</td>\n",
       "      <td>0.064529</td>\n",
       "      <td>0.063772</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>-0.017254</td>\n",
       "      <td>0.103314</td>\n",
       "      <td>-0.054453</td>\n",
       "      <td>0.025985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>-0.009858</td>\n",
       "      <td>-0.028317</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>-0.006525</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.005177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>-0.006016</td>\n",
       "      <td>-0.159820</td>\n",
       "      <td>-0.087673</td>\n",
       "      <td>0.063355</td>\n",
       "      <td>0.061493</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>-0.009578</td>\n",
       "      <td>0.093405</td>\n",
       "      <td>-0.052412</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016774</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.027015</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>-0.007556</td>\n",
       "      <td>-0.008190</td>\n",
       "      <td>-0.039200</td>\n",
       "      <td>-0.014289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7021</td>\n",
       "      <td>-0.007193</td>\n",
       "      <td>-0.155850</td>\n",
       "      <td>-0.086701</td>\n",
       "      <td>0.061205</td>\n",
       "      <td>0.059566</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>-0.008239</td>\n",
       "      <td>0.092551</td>\n",
       "      <td>-0.052765</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017100</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>-0.027081</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.041899</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>-0.008080</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.039849</td>\n",
       "      <td>-0.013918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7022</td>\n",
       "      <td>-0.010107</td>\n",
       "      <td>-0.146512</td>\n",
       "      <td>-0.083579</td>\n",
       "      <td>0.058978</td>\n",
       "      <td>0.057682</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>-0.049590</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021768</td>\n",
       "      <td>-0.011750</td>\n",
       "      <td>-0.032732</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>0.048995</td>\n",
       "      <td>0.020899</td>\n",
       "      <td>-0.014102</td>\n",
       "      <td>-0.027992</td>\n",
       "      <td>-0.061593</td>\n",
       "      <td>-0.026597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7023</td>\n",
       "      <td>-0.007071</td>\n",
       "      <td>-0.144043</td>\n",
       "      <td>-0.079855</td>\n",
       "      <td>0.057958</td>\n",
       "      <td>0.055704</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>-0.047519</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021990</td>\n",
       "      <td>-0.016278</td>\n",
       "      <td>-0.038178</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.052364</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>-0.019552</td>\n",
       "      <td>-0.038423</td>\n",
       "      <td>-0.073152</td>\n",
       "      <td>-0.032518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7024</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>-0.041755</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.031302</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>-0.016998</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>-0.032765</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.019534</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>-0.003051</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.003268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7025 rows × 299 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.009780 -0.153519 -0.086488  0.062050  0.059775  0.022163 -0.010749   \n",
       "1    -0.006704 -0.186385 -0.100420  0.074612  0.069428  0.019153 -0.011190   \n",
       "2    -0.014735 -0.137608 -0.079645  0.054923  0.049929  0.020018 -0.007845   \n",
       "3    -0.005637 -0.179852 -0.097126  0.074498  0.067334  0.021855 -0.013841   \n",
       "4    -0.004995 -0.159314 -0.082151  0.064529  0.063772  0.020675 -0.017254   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7020 -0.006016 -0.159820 -0.087673  0.063355  0.061493  0.019642 -0.009578   \n",
       "7021 -0.007193 -0.155850 -0.086701  0.061205  0.059566  0.018958 -0.008239   \n",
       "7022 -0.010107 -0.146512 -0.083579  0.058978  0.057682  0.021916 -0.009674   \n",
       "7023 -0.007071 -0.144043 -0.079855  0.057958  0.055704  0.021280 -0.011102   \n",
       "7024  0.005002 -0.041755 -0.001683  0.006120  0.031302  0.011992 -0.016998   \n",
       "\n",
       "             7         8         9  ...       289       290       291  \\\n",
       "0     0.089941 -0.053717  0.021990  ... -0.017769 -0.007670 -0.026404   \n",
       "1     0.112470 -0.060208  0.026991  ... -0.013348  0.000254 -0.021816   \n",
       "2     0.072751 -0.046585  0.016632  ... -0.027914 -0.013403 -0.042277   \n",
       "3     0.101287 -0.059853  0.024283  ... -0.015320 -0.006704 -0.028097   \n",
       "4     0.103314 -0.054453  0.025985  ... -0.004213 -0.004544 -0.009858   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7020  0.093405 -0.052412  0.023197  ... -0.016774 -0.006700 -0.027015   \n",
       "7021  0.092551 -0.052765  0.022502  ... -0.017100 -0.004267 -0.027081   \n",
       "7022  0.084256 -0.049590  0.021110  ... -0.021768 -0.011750 -0.032732   \n",
       "7023  0.075940 -0.047519  0.019985  ... -0.021990 -0.016278 -0.038178   \n",
       "7024  0.058780 -0.000466  0.012808  ...  0.004613 -0.032765 -0.002467   \n",
       "\n",
       "           292       293       294       295       296       297       298  \n",
       "0     0.004354  0.041168  0.013745 -0.009221 -0.013890 -0.045560 -0.018265  \n",
       "1    -0.016526  0.033347 -0.001565  0.000954  0.009685 -0.018930 -0.002832  \n",
       "2     0.038408  0.063280  0.033056 -0.023695 -0.052760 -0.087968 -0.039740  \n",
       "3    -0.001646  0.039481  0.007988 -0.008279 -0.008072 -0.039719 -0.015118  \n",
       "4    -0.028317  0.015599 -0.006525  0.005752  0.022966  0.001285  0.005177  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7020 -0.000707  0.040323  0.009394 -0.007556 -0.008190 -0.039200 -0.014289  \n",
       "7021  0.001552  0.041899  0.010162 -0.008080 -0.009752 -0.039849 -0.013918  \n",
       "7022  0.015828  0.048995  0.020899 -0.014102 -0.027992 -0.061593 -0.026597  \n",
       "7023  0.023187  0.052364  0.025306 -0.019552 -0.038423 -0.073152 -0.032518  \n",
       "7024 -0.019534  0.006022  0.003704 -0.003051  0.022616  0.006397  0.003268  \n",
       "\n",
       "[7025 rows x 299 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_res, y_res,test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9262, 299), (9262,), (4562, 299), (4562,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_res.shape,y_train_res.shape,X_test_res.shape,y_test_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=400,max_depth=8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = { \n",
    "    'n_estimators': [100,200,200,400,500],\n",
    "    #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [3,4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=RandomForestClassifier(max_depth=7, n_estimators=200,\n",
       "                                              random_state=0),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [3, 4, 5, 6, 7, 8],\n",
       "                         'n_estimators': [100, 200, 200, 400, 500]})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 2)\n",
    "CV_rfc.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 400}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, n_estimators=400, random_state=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9856402504858562"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787373958790004"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_res, y_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1367</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2\n",
       "0  1528     0     0\n",
       "1     5  1367    88\n",
       "2     0     4  1570"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_test_res, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randor_forest_classification report                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1528\n",
      "           1       1.00      0.94      0.97      1460\n",
      "           2       0.95      1.00      0.97      1574\n",
      "\n",
      "    accuracy                           0.98      4562\n",
      "   macro avg       0.98      0.98      0.98      4562\n",
      "weighted avg       0.98      0.98      0.98      4562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Randor_forest_classification report \",classification_report(y_test_res, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789337919174549"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9771186440677966"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2\n",
       "0  0     2  0\n",
       "1  0  2277  0\n",
       "2  0    47  0"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
