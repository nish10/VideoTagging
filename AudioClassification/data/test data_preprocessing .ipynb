{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import glob\n",
    "import moviepy.editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"D:\\video audio tag\\*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "chunk:   0%|                                                                        | 0/9497 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\video audio tag\\First US Presidential debate- the 'highlights'.mp4\n",
      "MoviePy - Writing audio in D:\\video audio tag\\First US Presidential debate- the 'highlights'.mp4_new_audio_.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in files:\n",
    "    video=moviepy.editor.VideoFileClip(i)\n",
    "    audio=video.audio\n",
    "    print(i)\n",
    "    path= i+\"_\"+'new_audio_.wav'\n",
    "    audio.write_audiofile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = glob.glob(r\"D:\\vioooo\\trial one\\*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\vioooo\\\\trial one\\\\0_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\1_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\2_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\3_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\4_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav',\n",
       " 'D:\\\\vioooo\\\\trial one\\\\5_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SplitWavAudioMubin():\n",
    "    def __init__(self, folder, filename):\n",
    "        self.folder = folder\n",
    "        self.filename = filename\n",
    "        self.filepath = folder + '\\\\' + filename\n",
    "        \n",
    "        self.audio = AudioSegment.from_wav(self.filepath)\n",
    "    \n",
    "    def get_duration(self):\n",
    "        return self.audio.duration_seconds\n",
    "    \n",
    "    def single_split(self, from_min, to_min, split_filename):\n",
    "        t1 = from_min * 60 * 1000\n",
    "        t2 = to_min * 60 * 1000\n",
    "        split_audio = self.audio[t1:t2]\n",
    "        split_audio.export(self.folder + '\\\\' + split_filename, format=\"wav\")\n",
    "        \n",
    "    def multiple_split(self, min_per_split):\n",
    "        total_mins = math.ceil(self.get_duration() / 60)\n",
    "        for i in range(0, total_mins, min_per_split):\n",
    "            split_fn = str(i) + '_' + self.filename\n",
    "            self.single_split(i, i+min_per_split, split_fn)\n",
    "            print(str(i) + ' Done')\n",
    "            if i == total_mins - min_per_split:\n",
    "                print('All splited successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Done\n",
      "1 Done\n",
      "2 Done\n",
      "3 Done\n",
      "4 Done\n",
      "5 Done\n",
      "6 Done\n",
      "7 Done\n",
      "All splited successfully\n"
     ]
    }
   ],
   "source": [
    "folder = r'D:\\video audio tag'\n",
    "file = 'audio_sample.wav'\n",
    "split_wav = SplitWavAudioMubin(folder, file)\n",
    "split_wav.multiple_split(min_per_split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# create a speech recognition object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition\n",
    "aud1=[]\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "                #aud1.append(whole_text)\n",
    "    aud1.append(whole_text)           \n",
    "    # return the text for all chunks detected\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\vioooo\\trial one\\0_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : I have great relationships with other people. \n",
      "audio-chunks\\chunk2.wav : I am released richest person in this world most precious travels you have a modern history. \n",
      "audio-chunks\\chunk3.wav : Post show on every single. \n",
      "audio-chunks\\chunk4.wav : Racist fire. \n",
      "audio-chunks\\chunk5.wav : Donald j trump. \n",
      "audio-chunks\\chunk6.wav : More and more people are. \n",
      "audio-chunks\\chunk7.wav : I getting better we have a problem that a worldwide problem this is a worldwide problem but i've been congratulated by as many countries on what we been able to do it will go away and as i say we've found in the turnover ravan in the cone is going over this is the same for this gun and easter rajasthan is the same for all who told you that even in this by the summer but it got dark winter. \n",
      "D:\\vioooo\\trial one\\1_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Twitter. \n",
      "audio-chunks\\chunk2.wav : He has no clear plan. \n",
      "audio-chunks\\chunk3.wav : Citation can be overwritten. \n",
      "audio-chunks\\chunk4.wav : People learning the diversity. \n",
      "audio-chunks\\chunk5.wav : Describe the black lives matter movement as a symbol of state. \n",
      "audio-chunks\\chunk6.wav : Shared variable and changing white power to millions of your support others you set the clock professional athlete exercising their first amendment rights should be fired usse do americans to say that kind of language is contributing to a climate of hay and ratio of strike i don't know what to say. \n",
      "audio-chunks\\chunk7.wav : I mean i can say anything. \n",
      "audio-chunks\\chunk8.wav : It's very it's mixed me. \n",
      "D:\\vioooo\\trial one\\2_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "Error: \n",
      "audio-chunks\\chunk2.wav : Because i am i am the least. \n",
      "audio-chunks\\chunk3.wav : Racist person i can even see the audience to show the earth. \n",
      "audio-chunks\\chunk4.wav : Ranu mandal list. \n",
      "audio-chunks\\chunk5.wav : Richest person in this world. \n",
      "audio-chunks\\chunk6.wav : Nobody has done more for the black. \n",
      "audio-chunks\\chunk7.wav : Comedi. \n",
      "audio-chunks\\chunk8.wav : With the exception of abraham lincoln except exception of abraham lincoln. \n",
      "audio-chunks\\chunk9.wav : Hippo show on every single. \n",
      "audio-chunks\\chunk10.wav : Racist fire. \n",
      "audio-chunks\\chunk11.wav : Abhishek bhagwan. \n",
      "audio-chunks\\chunk12.wav : It vary with trying very hard for this kit comes with the paracetamol with bottles in through origin. \n",
      "Error: \n",
      "audio-chunks\\chunk14.wav : Separate from the pant. \n",
      "audio-chunks\\chunk15.wav : Excess laughing stock in. \n",
      "audio-chunks\\chunk16.wav : Bible every notion of your. \n",
      "D:\\vioooo\\trial one\\3_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Nation. \n",
      "audio-chunks\\chunk2.wav : When you say the come back they don't come back you say never come back only the really. \n",
      "audio-chunks\\chunk3.wav : I said to say this. \n",
      "audio-chunks\\chunk4.wav : 50 with the lowest iq he might come back. \n",
      "audio-chunks\\chunk5.wav : Where is the problem when you run it it's no good so i'd like to terminate obamacare come up with a brand new beautiful health care costs the american people because disturb coronavirus in economics. \n",
      "audio-chunks\\chunk6.wav : Will you people have lost their private insurance. \n",
      "audio-chunks\\chunk7.wav : Anyone to check kuwait 22 million more people have another barmer care and over 110 billion people praising condition of people from covid-19 have pre-existing condition what are they gonna do. \n",
      "D:\\vioooo\\trial one\\4_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Agar achievers point 1 million on youtube i could find plan europe because the kind of things you've done in the kind of money is at your family is taken when you brother make money in a rani millions of dollars for the brother brother made of wood and its three years in the say you get some of it and you deliver value of hands all over the place you live. \n",
      "audio-chunks\\chunk2.wav : Mewati. \n",
      "audio-chunks\\chunk3.wav : Reservation for. \n",
      "audio-chunks\\chunk4.wav : Hidden want to talk about this this issue. \n",
      "audio-chunks\\chunk5.wav : His family. \n",
      "audio-chunks\\chunk6.wav : Your friend. \n",
      "audio-chunks\\chunk7.wav : You keep talking about these things should you do when you get there just a short time ago and you guys did nothing with their job and i really because i read because of barack obama for job. \n",
      "D:\\vioooo\\trial one\\5_Trump v Biden_ the key moments of the final presidential debate.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : Because i love u i look at you now your politician are run because i love you. \n",
      "audio-chunks\\chunk2.wav : Gopichand who became a because what happened in here is. \n",
      "audio-chunks\\chunk3.wav : You know who i am. \n",
      "audio-chunks\\chunk4.wav : You know who he was. \n",
      "audio-chunks\\chunk5.wav : You know which character you know my cat. \n",
      "audio-chunks\\chunk6.wav : Innova piche syster anarkali natak. \n",
      "audio-chunks\\chunk7.wav : I am anxious to have this race. \n",
      "audio-chunks\\chunk8.wav : I am anxious to see this take place. \n",
      "audio-chunks\\chunk9.wav : I am. \n",
      "audio-chunks\\chunk10.wav : Character of the countries on the ballot ok address book is costly. \n"
     ]
    }
   ],
   "source": [
    "for i in files1:\n",
    "    print(i)\n",
    "    get_large_audio_transcription(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "aud1=pd.DataFrame(aud1)\n",
    "aud1.replace(\"[^a-zA-Z.]\",\" \",regex=True,inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "aud1.replace(\"\", nan_value, inplace=True)\n",
    "aud1.dropna(axis=0,how='any',thresh=None,subset=None,inplace=True)\n",
    "data=aud1\n",
    "#len(data)\n",
    "headline=[]\n",
    "for row in range(0,len(data.index)):\n",
    "    headline.append(''.join(str(x) for x in data.iloc[row]))\n",
    "headline=str(headline)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline.lower()\n",
    "sentence=headline.split(\".\")\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "tokenText = []\n",
    "for sent in sentence:\n",
    "    tok = nltk.word_tokenize(sent)\n",
    "    if len(tok) > 0:\n",
    "        tokenText.append(tok)      \n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_sentence_new = [] \n",
    "filtered_sentence_new = [word for word in tokenText if not word in stopwords.words()]\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def wordlema(text):\n",
    "    lem_text=\" \".join([lemmatizer.lemmatize(i) for i in  text])\n",
    "    return lem_text\n",
    "lemtxt1 = []\n",
    "for tok in filtered_sentence_new:\n",
    "    lemSent = wordlema(tok)\n",
    "    lemtxt1.append(lemSent)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud2=pd.DataFrame(lemtxt1)\n",
    "aud2.replace(\"[^a-zA-Z.]\",\" \",regex=True,inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "aud2.replace(\"\", nan_value, inplace=True)\n",
    "aud2.dropna(axis=0,how='any',thresh=None,subset=None,inplace=True)\n",
    "data2=aud2\n",
    "#len(data)\n",
    "headline_new=[]\n",
    "for row in range(0,len(data2.index)):\n",
    "    headline_new.append(''.join(str(x) for x in data2.iloc[row]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hatesonar import Sonar\n",
    "sonar=Sonar()\n",
    "aa=[]\n",
    "for i in headline_new:\n",
    "    a=sonar.ping(text=i)\n",
    "    aa.append(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    I have great relationship with other people',\n",
       " 'I am released richest person in this world most precious travel you have a modern history',\n",
       " 'Post show on every single',\n",
       " 'Racist fire',\n",
       " 'Donald j trump',\n",
       " 'More and more people are',\n",
       " 'I getting better we have a problem that a worldwide problem this is a worldwide problem but i ve been congratulated by a many country on what we been able to do it will go away and a i say we ve found in the turnover ravan in the cone is going over this is the same for this gun and easter rajasthan is the same for all who told you that even in this by the summer but it got dark winter',\n",
       " '     Twitter',\n",
       " 'He ha no clear plan',\n",
       " 'Citation can be overwritten',\n",
       " 'People learning the diversity',\n",
       " 'Describe the black life matter movement a a symbol of state',\n",
       " 'Shared variable and changing white power to million of your support others you set the clock professional athlete exercising their first amendment right should be fired usse do american to say that kind of language is contributing to a climate of hay and ratio of strike i don t know what to say',\n",
       " 'I mean i can say anything',\n",
       " 'It s very it s mixed me',\n",
       " '     Because i am i am the least',\n",
       " 'Racist person i can even see the audience to show the earth',\n",
       " 'Ranu mandal list',\n",
       " 'Richest person in this world',\n",
       " 'Nobody ha done more for the black',\n",
       " 'Comedi',\n",
       " 'With the exception of abraham lincoln except exception of abraham lincoln',\n",
       " 'Hippo show on every single',\n",
       " 'Racist fire',\n",
       " 'Abhishek bhagwan',\n",
       " 'It vary with trying very hard for this kit come with the paracetamol with bottle in through origin',\n",
       " 'Separate from the pant',\n",
       " 'Excess laughing stock in',\n",
       " 'Bible every notion of your',\n",
       " '     Nation',\n",
       " 'When you say the come back they don t come back you say never come back only the really',\n",
       " 'I said to say this',\n",
       " 'with the lowest iq he might come back',\n",
       " 'Where is the problem when you run it it s no good so i d like to terminate obamacare come up with a brand new beautiful health care cost the american people because disturb coronavirus in economics',\n",
       " 'Will you people have lost their private insurance',\n",
       " 'Anyone to check kuwait million more people have another barmer care and over billion people praising condition of people from covid have pre existing condition what are they gon na do',\n",
       " '     Agar achiever point million on youtube i could find plan europe because the kind of thing you ve done in the kind of money is at your family is taken when you brother make money in a rani million of dollar for the brother brother made of wood and it three year in the say you get some of it and you deliver value of hand all over the place you live',\n",
       " 'Mewati',\n",
       " 'Reservation for',\n",
       " 'Hidden want to talk about this this issue',\n",
       " 'His family',\n",
       " 'Your friend',\n",
       " 'You keep talking about these thing should you do when you get there just a short time ago and you guy did nothing with their job and i really because i read because of barack obama for job',\n",
       " '     Because i love u i look at you now your politician are run because i love you',\n",
       " 'Gopichand who became a because what happened in here is',\n",
       " 'You know who i am',\n",
       " 'You know who he wa',\n",
       " 'You know which character you know my cat',\n",
       " 'Innova piche syster anarkali natak',\n",
       " 'I am anxious to have this race',\n",
       " 'I am anxious to see this take place',\n",
       " 'I am',\n",
       " 'Character of the country on the ballot ok address book is costly',\n",
       " '   ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnew=pd.DataFrame(aa)\n",
    "cnew.to_csv(r\"D:\\video audio tag\\hatesonar result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "pos_word_list=[]\n",
    "neu_word_list=[]\n",
    "neg_word_list=[]\n",
    "compound =[]\n",
    "for i in headline_new:\n",
    "    t= (analyzer.polarity_scores(i))['neg']\n",
    "    neg_word_list.append(t)\n",
    "    t= (analyzer.polarity_scores(i))['pos']\n",
    "    pos_word_list.append(t)\n",
    "    t= (analyzer.polarity_scores(i))['neu']\n",
    "    neu_word_list.append(t)\n",
    "    t= (analyzer.polarity_scores(i))['compound']\n",
    "    compound.append(t)\n",
    "df1 = pd.DataFrame({'neu': neu_word_list, 'pos': pos_word_list, 'neg': neg_word_list, 'com': compound}) \n",
    "df1['new'] = np.where(df1['com']>=.1, 'pos',\"neg\")\n",
    "#final = pd.concat([trainDataVecs,df1], axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r\"D:\\vioooo\\trial one\\polarity score for each sentence.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv(r\"C:\\Users\\pradeep\\text\\logggss.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=df2['Words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['propogate',\n",
       " 'striking',\n",
       " 'rocks',\n",
       " 'Gan',\n",
       " 'Kill',\n",
       " 'weapons',\n",
       " 'roll them back ',\n",
       " 'slapping',\n",
       " 'beating',\n",
       " 'persecuted',\n",
       " 'interjection',\n",
       " 'interject',\n",
       " 'kick',\n",
       " 'gan',\n",
       " 'beaten',\n",
       " 'weapons',\n",
       " 'kill',\n",
       " '2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted',\n",
       " 'aborts',\n",
       " 'abrade',\n",
       " 'abrasive',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abscond',\n",
       " 'absence',\n",
       " 'absent-minded',\n",
       " 'absentee',\n",
       " 'absurd',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'absurdness',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusive',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'accidental',\n",
       " 'accost',\n",
       " 'accursed',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accusingly',\n",
       " 'acerbate',\n",
       " 'acerbic',\n",
       " 'acerbically',\n",
       " 'ache',\n",
       " 'ached',\n",
       " 'aches',\n",
       " 'achey',\n",
       " 'aching',\n",
       " 'acrid',\n",
       " 'acridly',\n",
       " 'acridness',\n",
       " 'acrimonious',\n",
       " 'acrimoniously',\n",
       " 'acrimony',\n",
       " 'adamant',\n",
       " 'adamantly',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addicting',\n",
       " 'addicts',\n",
       " 'admonish',\n",
       " 'admonisher',\n",
       " 'admonishingly',\n",
       " 'admonishment',\n",
       " 'admonition',\n",
       " 'adulterate',\n",
       " 'adulterated',\n",
       " 'adulteration',\n",
       " 'adulterier',\n",
       " 'adversarial',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversity',\n",
       " 'afflict',\n",
       " 'affliction',\n",
       " 'afflictive',\n",
       " 'affront',\n",
       " 'afraid',\n",
       " 'aggravate',\n",
       " 'aggravating',\n",
       " 'aggravation',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressiveness',\n",
       " 'aggressor',\n",
       " 'aggrieve',\n",
       " 'aggrieved',\n",
       " 'aggrivation',\n",
       " 'aghast',\n",
       " 'agonies',\n",
       " 'agonize',\n",
       " 'agonizing',\n",
       " 'agonizingly',\n",
       " 'agony',\n",
       " 'aground',\n",
       " 'ail',\n",
       " 'ailing',\n",
       " 'ailment',\n",
       " 'aimless',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alarmingly',\n",
       " 'alienate',\n",
       " 'alienated',\n",
       " 'alienation',\n",
       " 'allegation',\n",
       " 'allegations',\n",
       " 'allege',\n",
       " 'allergic',\n",
       " 'allergies',\n",
       " 'allergy',\n",
       " 'aloof',\n",
       " 'altercation',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambivalence',\n",
       " 'ambivalent',\n",
       " 'ambush',\n",
       " 'amiss',\n",
       " 'amputate',\n",
       " 'anarchism',\n",
       " 'anarchist',\n",
       " 'anarchistic',\n",
       " 'anarchy',\n",
       " 'anemic',\n",
       " 'anger',\n",
       " 'angrily',\n",
       " 'angriness',\n",
       " 'angry',\n",
       " 'anguish',\n",
       " 'animosity',\n",
       " 'annihilate',\n",
       " 'annihilation',\n",
       " 'annoy',\n",
       " 'annoyance',\n",
       " 'annoyances',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annoyingly',\n",
       " 'annoys',\n",
       " 'anomalous',\n",
       " 'anomaly',\n",
       " 'antagonism',\n",
       " 'antagonist',\n",
       " 'antagonistic',\n",
       " 'antagonize',\n",
       " 'anti-',\n",
       " 'anti-american',\n",
       " 'anti-israeli',\n",
       " 'anti-occupation',\n",
       " 'anti-proliferation',\n",
       " 'anti-semites',\n",
       " 'anti-social',\n",
       " 'anti-us',\n",
       " 'anti-white',\n",
       " 'antipathy',\n",
       " 'antiquated',\n",
       " 'antithetical',\n",
       " 'anxieties',\n",
       " 'anxiety',\n",
       " 'anxious',\n",
       " 'anxiously',\n",
       " 'anxiousness',\n",
       " 'apathetic',\n",
       " 'apathetically',\n",
       " 'apathy',\n",
       " 'apocalypse',\n",
       " 'apocalyptic',\n",
       " 'apologist',\n",
       " 'apologists',\n",
       " 'appal',\n",
       " 'appall',\n",
       " 'appalled',\n",
       " 'appalling',\n",
       " 'appallingly',\n",
       " 'apprehension',\n",
       " 'apprehensions',\n",
       " 'apprehensive',\n",
       " 'apprehensively',\n",
       " 'arbitrary',\n",
       " 'arcane',\n",
       " 'archaic',\n",
       " 'arduous',\n",
       " 'arduously',\n",
       " 'argumentative',\n",
       " 'arrogance',\n",
       " 'arrogant',\n",
       " 'arrogantly',\n",
       " 'ashamed',\n",
       " 'asinine',\n",
       " 'asininely',\n",
       " 'asinininity',\n",
       " 'askance',\n",
       " 'asperse',\n",
       " 'aspersion',\n",
       " 'aspersions',\n",
       " 'assail',\n",
       " 'assassin',\n",
       " 'assassinate',\n",
       " 'assault',\n",
       " 'assult',\n",
       " 'astray',\n",
       " 'asunder',\n",
       " 'atrocious',\n",
       " 'atrocities',\n",
       " 'atrocity',\n",
       " 'atrophy',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'audacious',\n",
       " 'audaciously',\n",
       " 'audaciousness',\n",
       " 'audacity',\n",
       " 'audiciously',\n",
       " 'austere',\n",
       " 'authoritarian',\n",
       " 'autocrat',\n",
       " 'autocratic',\n",
       " 'avalanche',\n",
       " 'avarice',\n",
       " 'avaricious',\n",
       " 'avariciously',\n",
       " 'avenge',\n",
       " 'averse',\n",
       " 'aversion',\n",
       " 'aweful',\n",
       " 'awful',\n",
       " 'awfully',\n",
       " 'awfulness',\n",
       " 'awkward',\n",
       " 'awkwardness',\n",
       " 'ax',\n",
       " 'babble',\n",
       " 'back-logged',\n",
       " 'back-wood',\n",
       " 'back-woods',\n",
       " 'backache',\n",
       " 'backaches',\n",
       " 'backaching',\n",
       " 'backbite',\n",
       " 'backbiting',\n",
       " 'backward',\n",
       " 'backwardness',\n",
       " 'backwood',\n",
       " 'backwoods',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'baffle',\n",
       " 'baffled',\n",
       " 'bafflement',\n",
       " 'baffling',\n",
       " 'bait',\n",
       " 'balk',\n",
       " 'banal',\n",
       " 'banalize',\n",
       " 'bane',\n",
       " 'banish',\n",
       " 'banishment',\n",
       " 'bankrupt',\n",
       " 'barbarian',\n",
       " 'barbaric',\n",
       " 'barbarically',\n",
       " 'barbarity',\n",
       " 'barbarous',\n",
       " 'barbarously',\n",
       " 'barren',\n",
       " 'baseless',\n",
       " 'bash',\n",
       " 'bashed',\n",
       " 'bashful',\n",
       " 'bashing',\n",
       " 'bastard',\n",
       " 'bastards',\n",
       " 'battered',\n",
       " 'battering',\n",
       " 'batty',\n",
       " 'bearish',\n",
       " 'beastly',\n",
       " 'bedlam',\n",
       " 'bedlamite',\n",
       " 'befoul',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'beggarly',\n",
       " 'begging',\n",
       " 'beguile',\n",
       " 'belabor',\n",
       " 'belated',\n",
       " 'beleaguer',\n",
       " 'belie',\n",
       " 'belittle',\n",
       " 'belittled',\n",
       " 'belittling',\n",
       " 'bellicose',\n",
       " 'belligerence',\n",
       " 'belligerent',\n",
       " 'belligerently',\n",
       " 'bemoan',\n",
       " 'bemoaning',\n",
       " 'bemused',\n",
       " 'bent',\n",
       " 'berate',\n",
       " 'bereave',\n",
       " 'bereavement',\n",
       " 'bereft',\n",
       " 'berserk',\n",
       " 'beseech',\n",
       " 'beset',\n",
       " 'besiege',\n",
       " 'besmirch',\n",
       " 'bestial',\n",
       " 'betray',\n",
       " 'betrayal',\n",
       " 'betrayals',\n",
       " 'betrayer',\n",
       " 'betraying',\n",
       " 'betrays',\n",
       " 'bewail',\n",
       " 'beware',\n",
       " 'bewilder',\n",
       " 'bewildered',\n",
       " 'bewildering',\n",
       " 'bewilderingly',\n",
       " 'bewilderment',\n",
       " 'bewitch',\n",
       " 'bias',\n",
       " 'biased',\n",
       " 'biases',\n",
       " 'bicker',\n",
       " 'bickering',\n",
       " 'bid-rigging',\n",
       " 'bigotries',\n",
       " 'bigotry',\n",
       " 'bitch',\n",
       " 'bitchy',\n",
       " 'biting',\n",
       " 'bitingly',\n",
       " 'bitter',\n",
       " 'bitterly',\n",
       " 'bitterness',\n",
       " 'bizarre',\n",
       " 'blab',\n",
       " 'blabber',\n",
       " 'blackmail',\n",
       " 'blah',\n",
       " 'blame',\n",
       " 'blameworthy',\n",
       " 'bland',\n",
       " 'blandish',\n",
       " 'blaspheme',\n",
       " 'blasphemous',\n",
       " 'blasphemy',\n",
       " 'blasted',\n",
       " 'blatant',\n",
       " 'blatantly',\n",
       " 'blather',\n",
       " 'bleak',\n",
       " 'bleakly',\n",
       " 'bleakness',\n",
       " 'bleed',\n",
       " 'bleeding',\n",
       " 'bleeds',\n",
       " 'blemish',\n",
       " 'blind',\n",
       " 'blinding',\n",
       " 'blindingly',\n",
       " 'blindside',\n",
       " 'blister',\n",
       " 'blistering',\n",
       " 'bloated',\n",
       " 'blockage',\n",
       " 'blockhead',\n",
       " 'bloodshed',\n",
       " 'bloodthirsty',\n",
       " 'bloody',\n",
       " 'blotchy',\n",
       " 'blow',\n",
       " 'blunder',\n",
       " 'blundering',\n",
       " 'blunders',\n",
       " 'blunt',\n",
       " 'blur',\n",
       " 'bluring',\n",
       " 'blurred',\n",
       " 'blurring',\n",
       " 'blurry',\n",
       " 'blurs',\n",
       " 'blurt',\n",
       " 'boastful',\n",
       " 'boggle',\n",
       " 'bogus',\n",
       " 'boil',\n",
       " 'boiling',\n",
       " 'boisterous',\n",
       " 'bomb',\n",
       " 'bombard',\n",
       " 'bombardment',\n",
       " 'bombastic',\n",
       " 'bondage',\n",
       " 'bonkers',\n",
       " 'bore',\n",
       " 'bored',\n",
       " 'boredom',\n",
       " 'bores',\n",
       " 'boring',\n",
       " 'botch',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bothering',\n",
       " 'bothers',\n",
       " 'bothersome',\n",
       " 'bowdlerize',\n",
       " 'boycott',\n",
       " 'braggart',\n",
       " 'bragger',\n",
       " 'brainless',\n",
       " 'brainwash',\n",
       " 'brash',\n",
       " 'brashly',\n",
       " 'brashness',\n",
       " 'brat',\n",
       " 'bravado',\n",
       " 'brazen',\n",
       " 'brazenly',\n",
       " 'brazenness',\n",
       " 'breach',\n",
       " 'break',\n",
       " 'break-up',\n",
       " 'break-ups',\n",
       " 'breakdown',\n",
       " 'breaking',\n",
       " 'breaks',\n",
       " 'breakup',\n",
       " 'breakups',\n",
       " 'bribery',\n",
       " 'brimstone',\n",
       " 'bristle',\n",
       " 'brittle',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'broken-hearted',\n",
       " 'brood',\n",
       " 'browbeat',\n",
       " 'bruise',\n",
       " 'bruised',\n",
       " 'bruises',\n",
       " 'bruising',\n",
       " 'brusque',\n",
       " 'brutal',\n",
       " 'brutalising',\n",
       " 'brutalities',\n",
       " 'brutality',\n",
       " 'brutalize',\n",
       " 'brutalizing',\n",
       " 'brutally',\n",
       " 'brute',\n",
       " 'brutish',\n",
       " 'bs',\n",
       " 'buckle',\n",
       " 'bug',\n",
       " 'bugging',\n",
       " 'buggy',\n",
       " 'bugs',\n",
       " 'bulkier',\n",
       " 'bulkiness',\n",
       " 'bulky',\n",
       " 'bulkyness',\n",
       " 'bull****',\n",
       " 'bull----',\n",
       " 'bullies',\n",
       " 'bullshit',\n",
       " 'bullshyt',\n",
       " 'bully',\n",
       " 'bullying',\n",
       " 'bullyingly',\n",
       " 'bum',\n",
       " 'bump',\n",
       " 'bumped',\n",
       " 'bumping',\n",
       " 'bumpping',\n",
       " 'bumps',\n",
       " 'bumpy',\n",
       " 'bungle',\n",
       " 'bungler',\n",
       " 'bungling',\n",
       " 'bunk',\n",
       " 'burden',\n",
       " 'burdensome',\n",
       " 'burdensomely',\n",
       " 'burn',\n",
       " 'burned',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'bust',\n",
       " 'busts',\n",
       " 'busybody',\n",
       " 'butcher',\n",
       " 'butchery',\n",
       " 'buzzing',\n",
       " 'byzantine',\n",
       " 'cackle',\n",
       " 'calamities',\n",
       " 'calamitous',\n",
       " 'calamitously',\n",
       " 'calamity',\n",
       " 'callous',\n",
       " 'calumniate',\n",
       " 'calumniation',\n",
       " 'calumnies',\n",
       " 'calumnious',\n",
       " 'calumniously',\n",
       " 'calumny',\n",
       " 'cancer',\n",
       " 'cancerous',\n",
       " 'cannibal',\n",
       " 'cannibalize',\n",
       " 'capitulate',\n",
       " 'capricious',\n",
       " 'capriciously',\n",
       " 'capriciousness',\n",
       " 'capsize',\n",
       " 'careless',\n",
       " 'carelessness',\n",
       " 'caricature',\n",
       " 'carnage',\n",
       " 'carp',\n",
       " 'cartoonish',\n",
       " 'cash-strapped',\n",
       " 'castigate',\n",
       " 'castrated',\n",
       " 'casualty',\n",
       " 'cataclysm',\n",
       " 'cataclysmal',\n",
       " 'cataclysmic',\n",
       " 'cataclysmically',\n",
       " 'catastrophe',\n",
       " 'catastrophes',\n",
       " 'catastrophic',\n",
       " 'catastrophically',\n",
       " 'catastrophies',\n",
       " 'caustic',\n",
       " 'caustically',\n",
       " 'cautionary',\n",
       " 'cave',\n",
       " 'censure',\n",
       " 'chafe',\n",
       " 'chaff',\n",
       " 'chagrin',\n",
       " 'challenging',\n",
       " 'chaos',\n",
       " 'chaotic',\n",
       " 'chasten',\n",
       " 'chastise',\n",
       " 'chastisement',\n",
       " 'chatter',\n",
       " 'chatterbox',\n",
       " 'cheap',\n",
       " 'cheapen',\n",
       " 'cheaply',\n",
       " 'cheat',\n",
       " 'cheated',\n",
       " 'cheater',\n",
       " 'cheating',\n",
       " 'cheats',\n",
       " 'checkered',\n",
       " 'cheerless',\n",
       " 'cheesy',\n",
       " 'chide',\n",
       " 'childish',\n",
       " 'chill',\n",
       " 'chilly',\n",
       " 'chintzy',\n",
       " 'choke',\n",
       " 'choleric',\n",
       " 'choppy',\n",
       " 'chore',\n",
       " 'chronic',\n",
       " 'chunky',\n",
       " 'clamor',\n",
       " 'clamorous',\n",
       " 'clash',\n",
       " 'cliche',\n",
       " 'cliched',\n",
       " 'clique',\n",
       " 'clog',\n",
       " 'clogged',\n",
       " 'clogs',\n",
       " 'cloud',\n",
       " 'clouding',\n",
       " 'cloudy',\n",
       " 'clueless',\n",
       " 'clumsy',\n",
       " 'clunky',\n",
       " 'coarse',\n",
       " 'cocky',\n",
       " 'coerce',\n",
       " 'coercion',\n",
       " 'coercive',\n",
       " 'cold',\n",
       " 'coldly',\n",
       " 'collapse',\n",
       " 'collude',\n",
       " 'collusion',\n",
       " 'combative',\n",
       " 'combust',\n",
       " 'comical',\n",
       " 'commiserate',\n",
       " 'commonplace',\n",
       " 'commotion',\n",
       " 'commotions',\n",
       " 'complacent',\n",
       " 'complain',\n",
       " 'complained',\n",
       " 'complaining',\n",
       " 'complains',\n",
       " 'complaint',\n",
       " 'complaints',\n",
       " 'complex',\n",
       " 'complicated',\n",
       " 'complication',\n",
       " 'complicit',\n",
       " 'compulsion',\n",
       " 'compulsive',\n",
       " 'concede',\n",
       " 'conceded',\n",
       " 'conceit',\n",
       " 'conceited',\n",
       " 'concen',\n",
       " 'concens',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'concerns',\n",
       " 'concession',\n",
       " 'concessions',\n",
       " 'condemn',\n",
       " 'condemnable',\n",
       " 'condemnation',\n",
       " 'condemned',\n",
       " 'condemns',\n",
       " 'condescend',\n",
       " 'condescending',\n",
       " 'condescendingly',\n",
       " 'condescension',\n",
       " 'confess',\n",
       " 'confession',\n",
       " 'confessions',\n",
       " 'confined',\n",
       " 'conflict',\n",
       " 'conflicted',\n",
       " 'conflicting',\n",
       " 'conflicts',\n",
       " 'confound',\n",
       " 'confounded',\n",
       " 'confounding',\n",
       " 'confront',\n",
       " 'confrontation',\n",
       " 'confrontational',\n",
       " 'confuse',\n",
       " 'confused',\n",
       " 'confuses',\n",
       " 'confusing',\n",
       " 'confusion',\n",
       " 'confusions',\n",
       " 'congested',\n",
       " 'congestion',\n",
       " 'cons',\n",
       " 'conscons',\n",
       " 'conservative',\n",
       " 'conspicuous',\n",
       " 'conspicuously',\n",
       " 'conspiracies',\n",
       " 'conspiracy',\n",
       " 'conspirator',\n",
       " 'conspiratorial',\n",
       " 'conspire',\n",
       " 'consternation',\n",
       " 'contagious',\n",
       " 'contaminate',\n",
       " 'contaminated',\n",
       " 'contaminates',\n",
       " 'contaminating',\n",
       " 'contamination',\n",
       " 'contempt',\n",
       " 'contemptible',\n",
       " 'contemptuous',\n",
       " 'contemptuously',\n",
       " 'contend',\n",
       " 'contention',\n",
       " 'contentious',\n",
       " 'contort',\n",
       " 'contortions',\n",
       " 'contradict',\n",
       " 'contradiction',\n",
       " 'contradictory',\n",
       " 'contrariness',\n",
       " 'contravene',\n",
       " 'contrive',\n",
       " 'contrived',\n",
       " 'controversial',\n",
       " 'controversy',\n",
       " 'convoluted',\n",
       " 'corrode',\n",
       " 'corrosion',\n",
       " 'corrosions',\n",
       " 'corrosive',\n",
       " 'corrupt',\n",
       " 'corrupted',\n",
       " 'corrupting',\n",
       " 'corruption',\n",
       " 'corrupts',\n",
       " 'corruptted',\n",
       " 'costlier',\n",
       " 'costly',\n",
       " 'counter-productive',\n",
       " 'counterproductive',\n",
       " 'coupists',\n",
       " 'covetous',\n",
       " 'coward',\n",
       " 'cowardly',\n",
       " 'crabby',\n",
       " 'crack',\n",
       " 'cracked',\n",
       " 'cracks',\n",
       " 'craftily',\n",
       " 'craftly',\n",
       " 'crafty',\n",
       " 'cramp',\n",
       " 'cramped',\n",
       " 'cramping',\n",
       " 'cranky',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'craps',\n",
       " 'crash',\n",
       " 'crashed',\n",
       " 'crashes',\n",
       " 'crashing',\n",
       " 'crass',\n",
       " 'craven',\n",
       " 'cravenly',\n",
       " 'craze',\n",
       " 'crazily',\n",
       " 'craziness',\n",
       " 'crazy',\n",
       " 'creak',\n",
       " 'creaking',\n",
       " 'creaks',\n",
       " 'credulous',\n",
       " 'creep',\n",
       " 'creeping',\n",
       " 'creeps',\n",
       " 'creepy',\n",
       " 'crept',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'cringe',\n",
       " 'cringed',\n",
       " 'cringes',\n",
       " 'cripple',\n",
       " 'crippled',\n",
       " 'cripples',\n",
       " 'crippling',\n",
       " 'crisis',\n",
       " 'critic',\n",
       " 'critical',\n",
       " 'criticism',\n",
       " 'criticisms',\n",
       " 'criticize',\n",
       " 'criticized',\n",
       " 'criticizing',\n",
       " 'critics',\n",
       " 'cronyism',\n",
       " 'crook',\n",
       " 'crooked',\n",
       " 'crooks',\n",
       " 'crowded',\n",
       " 'crowdedness',\n",
       " 'crude',\n",
       " 'cruel',\n",
       " 'crueler',\n",
       " 'cruelest',\n",
       " 'cruelly',\n",
       " 'cruelness',\n",
       " 'cruelties',\n",
       " 'cruelty',\n",
       " 'crumble',\n",
       " 'crumbling',\n",
       " 'crummy',\n",
       " 'crumple',\n",
       " 'crumpled',\n",
       " 'crumples',\n",
       " 'crush',\n",
       " 'crushed',\n",
       " 'crushing',\n",
       " 'cry',\n",
       " 'culpable',\n",
       " 'culprit',\n",
       " 'cumbersome',\n",
       " 'cunt',\n",
       " 'cunts',\n",
       " 'cuplrit',\n",
       " 'curse',\n",
       " 'cursed',\n",
       " 'curses',\n",
       " 'curt',\n",
       " 'cuss',\n",
       " 'cussed',\n",
       " 'cutthroat',\n",
       " 'cynical',\n",
       " 'cynicism',\n",
       " 'd*mn',\n",
       " 'damage',\n",
       " 'damaged',\n",
       " 'damages',\n",
       " 'damaging',\n",
       " 'damn',\n",
       " 'damnable',\n",
       " 'damnably',\n",
       " 'damnation',\n",
       " 'damned',\n",
       " 'damning',\n",
       " 'damper',\n",
       " 'danger',\n",
       " 'dangerous',\n",
       " 'dangerousness',\n",
       " 'dark',\n",
       " 'darken',\n",
       " 'darkened',\n",
       " 'darker',\n",
       " 'darkness',\n",
       " 'dastard',\n",
       " 'dastardly',\n",
       " 'daunt',\n",
       " 'daunting',\n",
       " 'dauntingly',\n",
       " 'dawdle',\n",
       " 'daze',\n",
       " 'dazed',\n",
       " 'dead',\n",
       " 'deadbeat',\n",
       " 'deadlock',\n",
       " 'deadly',\n",
       " 'deadweight',\n",
       " 'deaf',\n",
       " 'dearth',\n",
       " 'death',\n",
       " 'debacle',\n",
       " 'debase',\n",
       " 'debasement',\n",
       " 'debaser',\n",
       " 'debatable',\n",
       " 'debauch',\n",
       " 'debaucher',\n",
       " 'debauchery',\n",
       " 'debilitate',\n",
       " 'debilitating',\n",
       " 'debility',\n",
       " 'debt',\n",
       " 'debts',\n",
       " 'decadence',\n",
       " 'decadent',\n",
       " 'decay',\n",
       " 'decayed',\n",
       " 'deceit',\n",
       " 'deceitful',\n",
       " 'deceitfully',\n",
       " 'deceitfulness',\n",
       " 'deceive',\n",
       " 'deceiver',\n",
       " 'deceivers',\n",
       " 'deceiving',\n",
       " 'deception',\n",
       " 'deceptive',\n",
       " 'deceptively',\n",
       " 'declaim',\n",
       " 'decline',\n",
       " 'declines',\n",
       " 'declining',\n",
       " 'decrement',\n",
       " 'decrepit',\n",
       " 'decrepitude',\n",
       " 'decry',\n",
       " 'defamation',\n",
       " 'defamations',\n",
       " 'defamatory',\n",
       " 'defame',\n",
       " 'defect',\n",
       " 'defective',\n",
       " 'defects',\n",
       " 'defensive',\n",
       " 'defiance',\n",
       " 'defiant',\n",
       " 'defiantly',\n",
       " 'deficiencies',\n",
       " 'deficiency',\n",
       " 'deficient',\n",
       " 'defile',\n",
       " 'defiler',\n",
       " 'deform',\n",
       " 'deformed',\n",
       " 'defrauding',\n",
       " 'defunct',\n",
       " 'defy',\n",
       " 'degenerate',\n",
       " 'degenerately',\n",
       " 'degeneration',\n",
       " 'degradation',\n",
       " 'degrade',\n",
       " 'degrading',\n",
       " 'degradingly',\n",
       " 'dehumanization',\n",
       " 'dehumanize',\n",
       " 'deign',\n",
       " 'deject',\n",
       " 'dejected',\n",
       " 'dejectedly',\n",
       " 'dejection',\n",
       " 'delay',\n",
       " 'delayed',\n",
       " 'delaying',\n",
       " 'delays',\n",
       " 'delinquency',\n",
       " 'delinquent',\n",
       " 'delirious',\n",
       " 'delirium',\n",
       " 'delude',\n",
       " 'deluded',\n",
       " 'deluge',\n",
       " 'delusion',\n",
       " 'delusional',\n",
       " 'delusions',\n",
       " 'demean',\n",
       " 'demeaning',\n",
       " 'demise',\n",
       " 'demolish',\n",
       " 'demolisher',\n",
       " 'demon',\n",
       " 'demonic',\n",
       " 'demonize',\n",
       " 'demonized',\n",
       " 'demonizes',\n",
       " 'demonizing',\n",
       " 'demoralize',\n",
       " 'demoralizing',\n",
       " 'demoralizingly',\n",
       " 'denial',\n",
       " 'denied',\n",
       " 'denies',\n",
       " 'denigrate',\n",
       " 'denounce',\n",
       " 'dense',\n",
       " 'dent',\n",
       " 'dented',\n",
       " 'dents',\n",
       " 'denunciate',\n",
       " 'denunciation',\n",
       " 'denunciations',\n",
       " 'deny',\n",
       " 'denying',\n",
       " 'deplete',\n",
       " 'deplorable',\n",
       " 'deplorably',\n",
       " 'deplore',\n",
       " 'deploring',\n",
       " 'deploringly',\n",
       " 'deprave',\n",
       " 'depraved',\n",
       " 'depravedly',\n",
       " 'deprecate',\n",
       " 'depress',\n",
       " 'depressed',\n",
       " 'depressing',\n",
       " 'depressingly',\n",
       " 'depression',\n",
       " 'depressions',\n",
       " 'deprive',\n",
       " 'deprived',\n",
       " 'deride',\n",
       " 'derision',\n",
       " 'derisive',\n",
       " 'derisively',\n",
       " 'derisiveness',\n",
       " 'derogatory',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "def check_Violating(text_to_check):\n",
    "    if set(new).intersection(set(text_to_check.split())):\n",
    "        print(\"Violating Word  Alert!!\")\n",
    "        res.append('Violating Word  Alert!!')\n",
    "        \n",
    "    else:\n",
    "        print(\" No Violating Word  Alert!!\")\n",
    "        res.append('No Violating Word  Alert!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n",
      "Violating Word  Alert!!\n",
      " No Violating Word  Alert!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in headline_new:\n",
    "    check_Violating(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame(res)\n",
    "res.to_csv(r\"D:\\vioooo\\trial one\\word matching result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '   Amazon vice president',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'Abhi positioner',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'I got drylock a military lucky tonight',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03584079736248833},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.34893812420252673},\n",
       "   {'class_name': 'neither', 'confidence': 0.6152210784349851}]},\n",
       " {'text': 'Because there',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03936808375765117},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.2611294070548147},\n",
       "   {'class_name': 'neither', 'confidence': 0.6995025091875342}]},\n",
       " {'text': 'Is there',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.058135835131279276},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.24227746891369034},\n",
       "   {'class_name': 'neither', 'confidence': 0.6995866959550303}]},\n",
       " {'text': 'Factors that everything is essential for it simple like everybody know is a liar i just want to make sure',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04507387766546182},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.4220022956932841},\n",
       "   {'class_name': 'neither', 'confidence': 0.532923826641254}]},\n",
       " {'text': '     Dad',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'He ha it now',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04429240834946092},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.32871166531607465},\n",
       "   {'class_name': 'neither', 'confidence': 0.6269959263344643}]},\n",
       " {'text': 'You agreed with bernie sander on a plane have any idea what this time don t you have little people now',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.06783520490611261},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.43362792731234906},\n",
       "   {'class_name': 'neither', 'confidence': 0.4985368677815383}]},\n",
       " {'text': 'Request because because the question is your sister',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.06553996986995349},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3113926866911046},\n",
       "   {'class_name': 'neither', 'confidence': 0.6230673434389418}]},\n",
       " {'text': 'Alleppey port',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'What more can a diversity get relax model',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04457318806937966},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3235951286342711},\n",
       "   {'class_name': 'neither', 'confidence': 0.6318316832963491}]},\n",
       " {'text': 'Use the word smart',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.013132242539675768},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.40710399509828077},\n",
       "   {'class_name': 'neither', 'confidence': 0.5797637623620433}]},\n",
       " {'text': '     You want to deliver state but you forgot the name of your college',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.13115185963310644},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.29987071947885074},\n",
       "   {'class_name': 'neither', 'confidence': 0.5689774208880428}]},\n",
       " {'text': 'Tiranga delaware state he graduated',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.07905198712968023},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.23180253379825116},\n",
       "   {'class_name': 'neither', 'confidence': 0.6891454790720686}]},\n",
       " {'text': 'Eid gulliver story almost lost in yesterday we use the word smart with me tell you that would give you because you know what is nothing smart bed youtube year you done this to be reviewed and had limited',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.041484814514544936},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.47026293774048955},\n",
       "   {'class_name': 'neither', 'confidence': 0.4882522477449655}]},\n",
       " {'text': 'You are a bad',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.16435767179206096},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3244253780753213},\n",
       "   {'class_name': 'neither', 'confidence': 0.5112169501326177}]},\n",
       " {'text': 'The church wa put to close the greatest economy in the history of our country and by the way now wa being that again',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03872685659369611},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.34226747100443267},\n",
       "   {'class_name': 'neither', 'confidence': 0.6190056724018712}]},\n",
       " {'text': 'How doe the pressure and asking you question',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.049218277170838196},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.317997902014592},\n",
       "   {'class_name': 'neither', 'confidence': 0.6327838208145699}]},\n",
       " {'text': 'Will you tell u how much you paid in federal income tax',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.05807932504166992},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3744309050393159},\n",
       "   {'class_name': 'neither', 'confidence': 0.5674897699190141}]},\n",
       " {'text': 'Infinix and million of dollar',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03865112825400118},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.28972563527262535},\n",
       "   {'class_name': 'neither', 'confidence': 0.6716232364733734}]},\n",
       " {'text': 'Recharge tech and that s why i am not limited to trump text',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03009785738561651},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.39076679139260223},\n",
       "   {'class_name': 'neither', 'confidence': 0.5791353512217813}]},\n",
       " {'text': 'What is mr robot',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.05256054338306656},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.32402999693265205},\n",
       "   {'class_name': 'neither', 'confidence': 0.6234094596842813}]},\n",
       " {'text': '     You want president story thing up the worst president ever had in',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04687661643280098},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.36333341360844273},\n",
       "   {'class_name': 'neither', 'confidence': 0.5897899699587562}]},\n",
       " {'text': 'In month i ve done more than you ve done it year here',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.033011858758504466},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3255429321430793},\n",
       "   {'class_name': 'neither', 'confidence': 0.6414452090984163}]},\n",
       " {'text': 'Skycon during this extended period not to engage in any discipline rest and value flight tonight',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04017897766007176},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3015697940832899},\n",
       "   {'class_name': 'neither', 'confidence': 0.6582512282566384}]},\n",
       " {'text': 'What you want not declare object array until the election ha been independently certified',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.050403530436602124},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3469023739539796},\n",
       "   {'class_name': 'neither', 'confidence': 0.6026940956094182}]},\n",
       " {'text': 'Person from the go into the pole and watch very carefully because that s what ha happened i am urging them to do it',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.05290734450164685},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3013434886371732},\n",
       "   {'class_name': 'neither', 'confidence': 0.6457491668611799}]},\n",
       " {'text': 'As you know today it wa a big problem in philadelphia dive into whatever pic oppo what is a very safe and i think they were thrown at wadala to watch you know why is bad thing happen if the weather of your pair thing',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04602291495227331},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.395214130134604},\n",
       "   {'class_name': 'neither', 'confidence': 0.5587629549131228}]},\n",
       " {'text': 'An emerging i am majhi my paper i hope it will be fair election in which affair',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04975417987392718},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.29946950943293255},\n",
       "   {'class_name': 'neither', 'confidence': 0.6507763106931402}]},\n",
       " {'text': '      I am depression on put water wisely ten of thousand a balanced being manipulated i can go along with and operator',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03086113581196869},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3478724441967661},\n",
       "   {'class_name': 'neither', 'confidence': 0.6212664199912651}]},\n",
       " {'text': 'Pipla request number number',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'Hey sound balance in a waste paper basket',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'day ago and had the name of the name from poonam',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.038824443705361186},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.26105321347653115},\n",
       "   {'class_name': 'neither', 'confidence': 0.7001223428181077}]},\n",
       " {'text': 'The discounted and give you played',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.056885695386555996},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.309427251130919},\n",
       "   {'class_name': 'neither', 'confidence': 0.6336870534825251}]},\n",
       " {'text': 'Not declare object to reenter the election is independent',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04858458631484341},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3175679527721986},\n",
       "   {'class_name': 'neither', 'confidence': 0.633847460912958}]},\n",
       " {'text': 'Yes',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'What s the day',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.048699046654297765},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.2728968630785913},\n",
       "   {'class_name': 'neither', 'confidence': 0.6784040902671109}]},\n",
       " {'text': 'Outbound point cloud',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'Summaries power system state can even be opened until election day',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.05723772214950354},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.27004450084884496},\n",
       "   {'class_name': 'neither', 'confidence': 0.6727177770016515}]},\n",
       " {'text': 'balance take time to do it',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04435883712141674},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.34432048452331526},\n",
       "   {'class_name': 'neither', 'confidence': 0.611320678355268}]},\n",
       " {'text': 'Diamond boring',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': '     Balance',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'Enough',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': 'That s a nice what happened happened why wa it not why is it for them some have not fortune',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.02981114541013507},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.35648001594739137},\n",
       "   {'class_name': 'neither', 'confidence': 0.6137088386424736}]},\n",
       " {'text': 'Same process',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech',\n",
       "    'confidence': 0.040463746574850266},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3395841185502645},\n",
       "   {'class_name': 'neither', 'confidence': 0.6199521348748852}]},\n",
       " {'text': 'Is honest no one is established and all that there is',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.05252256551484362},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.36773815888412936},\n",
       "   {'class_name': 'neither', 'confidence': 0.5797392756010269}]},\n",
       " {'text': 'Fraud related to mail in ballot',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.04379695572212084},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.32780990470689875},\n",
       "   {'class_name': 'neither', 'confidence': 0.6283931395709804}]},\n",
       " {'text': 'Are you can i take them white supremacist and militia group and that they need to stand down and not add to the violence and the number of the history of mi store in kenosha and suite in important',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.19462908965411732},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3006650205396649},\n",
       "   {'class_name': 'neither', 'confidence': 0.5047058898062179}]},\n",
       " {'text': 'You want to call him',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.0771160825460438},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.3856325545791324},\n",
       "   {'class_name': 'neither', 'confidence': 0.5372513628748239}]},\n",
       " {'text': 'What you want to kollam give me your name give me what is the best wi fi stand back insane by one of the you what',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.0764261997767706},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.4013752124389841},\n",
       "   {'class_name': 'neither', 'confidence': 0.5221985877842452}]},\n",
       " {'text': '     You had something and the problem is',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.066074677865735},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.30822360400765403},\n",
       "   {'class_name': 'neither', 'confidence': 0.6257017181266109}]},\n",
       " {'text': 'Aankada',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]},\n",
       " {'text': '   ',\n",
       "  'top_class': 'neither',\n",
       "  'classes': [{'class_name': 'hate_speech', 'confidence': 0.03945320844208391},\n",
       "   {'class_name': 'offensive_language', 'confidence': 0.33037608806864915},\n",
       "   {'class_name': 'neither', 'confidence': 0.630170703489267}]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnew=pd.DataFrame(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>top_class</th>\n",
       "      <th>classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Amazon vice president</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Abhi positioner</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I got drylock a military lucky tonight</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Because there</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Is there</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Factors that everything is essential for it si...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Dad</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>He ha it now</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>You agreed with bernie sander on a plane have ...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Request because because the question is your s...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Alleppey port</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>What more can a diversity get relax model</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Use the word smart</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>You want to deliver state but you forgot ...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Tiranga delaware state he graduated</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Eid gulliver story almost lost in yesterday we...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>You are a bad</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>The church wa put to close the greatest econom...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>How doe the pressure and asking you question</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Will you tell u how much you paid in federal i...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Infinix and million of dollar</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Recharge tech and that s why i am not limited ...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>What is mr robot</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>You want president story thing up the wor...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>In month i ve done more than you ve done it ye...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Skycon during this extended period not to enga...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>What you want not declare object array until t...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Person from the go into the pole and watch ver...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>As you know today it wa a big problem in phila...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>An emerging i am majhi my paper i hope it will...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>I am depression on put water wisely ten ...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>Pipla request number number</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>Hey sound balance in a waste paper basket</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>day ago and had the name of the name from poonam</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>The discounted and give you played</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>Not declare object to reenter the election is ...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>Yes</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>What s the day</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>Outbound point cloud</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>Summaries power system state can even be opene...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>balance take time to do it</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>Diamond boring</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>Balance</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>Enough</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>That s a nice what happened happened why wa it...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>Same process</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>Is honest no one is established and all that t...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>Fraud related to mail in ballot</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>Are you can i take them white supremacist and ...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>You want to call him</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>What you want to kollam give me your name give...</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>You had something and the problem is</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>Aankada</td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td></td>\n",
       "      <td>neither</td>\n",
       "      <td>[{'class_name': 'hate_speech', 'confidence': 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text top_class  \\\n",
       "0                               Amazon vice president   neither   \n",
       "1                                     Abhi positioner   neither   \n",
       "2              I got drylock a military lucky tonight   neither   \n",
       "3                                       Because there   neither   \n",
       "4                                            Is there   neither   \n",
       "5   Factors that everything is essential for it si...   neither   \n",
       "6                                                 Dad   neither   \n",
       "7                                        He ha it now   neither   \n",
       "8   You agreed with bernie sander on a plane have ...   neither   \n",
       "9   Request because because the question is your s...   neither   \n",
       "10                                      Alleppey port   neither   \n",
       "11          What more can a diversity get relax model   neither   \n",
       "12                                 Use the word smart   neither   \n",
       "13       You want to deliver state but you forgot ...   neither   \n",
       "14                Tiranga delaware state he graduated   neither   \n",
       "15  Eid gulliver story almost lost in yesterday we...   neither   \n",
       "16                                      You are a bad   neither   \n",
       "17  The church wa put to close the greatest econom...   neither   \n",
       "18       How doe the pressure and asking you question   neither   \n",
       "19  Will you tell u how much you paid in federal i...   neither   \n",
       "20                      Infinix and million of dollar   neither   \n",
       "21  Recharge tech and that s why i am not limited ...   neither   \n",
       "22                                   What is mr robot   neither   \n",
       "23       You want president story thing up the wor...   neither   \n",
       "24  In month i ve done more than you ve done it ye...   neither   \n",
       "25  Skycon during this extended period not to enga...   neither   \n",
       "26  What you want not declare object array until t...   neither   \n",
       "27  Person from the go into the pole and watch ver...   neither   \n",
       "28  As you know today it wa a big problem in phila...   neither   \n",
       "29  An emerging i am majhi my paper i hope it will...   neither   \n",
       "30        I am depression on put water wisely ten ...   neither   \n",
       "31                        Pipla request number number   neither   \n",
       "32          Hey sound balance in a waste paper basket   neither   \n",
       "33   day ago and had the name of the name from poonam   neither   \n",
       "34                 The discounted and give you played   neither   \n",
       "35  Not declare object to reenter the election is ...   neither   \n",
       "36                                                Yes   neither   \n",
       "37                                     What s the day   neither   \n",
       "38                               Outbound point cloud   neither   \n",
       "39  Summaries power system state can even be opene...   neither   \n",
       "40                         balance take time to do it   neither   \n",
       "41                                     Diamond boring   neither   \n",
       "42                                            Balance   neither   \n",
       "43                                             Enough   neither   \n",
       "44  That s a nice what happened happened why wa it...   neither   \n",
       "45                                       Same process   neither   \n",
       "46  Is honest no one is established and all that t...   neither   \n",
       "47                    Fraud related to mail in ballot   neither   \n",
       "48  Are you can i take them white supremacist and ...   neither   \n",
       "49                               You want to call him   neither   \n",
       "50  What you want to kollam give me your name give...   neither   \n",
       "51               You had something and the problem is   neither   \n",
       "52                                            Aankada   neither   \n",
       "53                                                      neither   \n",
       "\n",
       "                                              classes  \n",
       "0   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "1   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "2   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "3   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "4   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "5   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "6   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "7   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "8   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "9   [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "10  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "11  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "12  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "13  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "14  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "15  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "16  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "17  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "18  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "19  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "20  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "21  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "22  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "23  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "24  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "25  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "26  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "27  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "28  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "29  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "30  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "31  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "32  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "33  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "34  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "35  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "36  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "37  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "38  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "39  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "40  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "41  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "42  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "43  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "44  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "45  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "46  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "47  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "48  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "49  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "50  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "51  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "52  [{'class_name': 'hate_speech', 'confidence': 0...  \n",
       "53  [{'class_name': 'hate_speech', 'confidence': 0...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnew.to_csv(r\"D:\\video audio tag\\hatesonar result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect=cnew['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import csv\n",
    "import docx2txt\n",
    "import numpy\n",
    "# used for looping through folders/files\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "#Calc tfidf and cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_score(trainDataVecs):\n",
    "    d=[]\n",
    "    dd=[]\n",
    "    for i in range(0,len(trainDataVecs)):\n",
    "        for j in range(i+1,len(trainDataVecs)):\n",
    "            a=cosine_similarity([trainDataVecs[i]], [trainDataVecs[j]])\n",
    "            #anew=max(a)\n",
    "            d.append(a)\n",
    "    return d\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed) 0 as per num_features, in our case 300\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)  # words known to the model which we added here 10500 around \n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1\n",
    "            feature_vec = np.add(feature_vec,model[word]) # Calculate the weights if got word \n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords) # divide the weights according to  words  by divinding them number of words in sentence\n",
    "    return feature_vec\n",
    "def get_avg_feature_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Calculate average feature vectors f      b  or all reviews\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n",
    "    \n",
    "    for review in reviews:\n",
    "        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return review_feature_vecs    \n",
    "def max_similarity1(tfs,fileNames):\n",
    "    maxi=0\n",
    "    ids=1\n",
    "    for i in range(0,44):\n",
    "        for j in range(i+1,44):\n",
    "            a=cosine_similarity([trainDataVecs[i]], [trainDataVecs[j]])\n",
    "            a=a[0][0]\n",
    "            #print(str(i+1)+ \" \"+str(a))\n",
    "        #print(value + \",\" + i)\n",
    "            if a>maxi:\n",
    "                ids=i\n",
    "                maxi=a\n",
    "    return [ids+1,maxi]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import docx2txt\n",
    "import glob\n",
    "from langdetect import detect\n",
    "from translate import Translator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import os\n",
    "model_name = 'train_model'\n",
    "# Set values for various word2vec parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 3       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "if not os.path.exists(model_name): \n",
    "    # Initialize and train the model (this will take some time)\n",
    "    model = Word2Vec(filtered_sentence, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "\n",
    "    # If you don't plan to train the model any further, calling \n",
    "    # init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "\n",
    "    # It can be helpful to create a meaningful model name and \n",
    "    # save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model.save(model_name)\n",
    "else:\n",
    "    model = Word2Vec.load(model_name)\n",
    "    \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs = get_avg_feature_vecs(vect, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0097797 , -0.1535192 , -0.08648782, ..., -0.04555966,\n",
       "        -0.0182653 , -0.00650879],\n",
       "       [-0.00670387, -0.1863848 , -0.10041971, ..., -0.01892955,\n",
       "        -0.00283211,  0.01212567],\n",
       "       [-0.01473476, -0.13760836, -0.07964463, ..., -0.08796848,\n",
       "        -0.03973971, -0.03026342],\n",
       "       ...,\n",
       "       [-0.01010666, -0.14651187, -0.08357932, ..., -0.06159288,\n",
       "        -0.02659735, -0.01571397],\n",
       "       [-0.0070706 , -0.14404333, -0.07985536, ..., -0.07315195,\n",
       "        -0.0325175 , -0.02268166],\n",
       "       [ 0.00500207, -0.04175455, -0.00168312, ...,  0.00639704,\n",
       "         0.00326777,  0.01193533]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1=pd.DataFrame(trainDataVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1=trainDataVecs1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataVecs1.to_csv(\"trainDataVecs1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.009780</td>\n",
       "      <td>-0.153519</td>\n",
       "      <td>-0.086488</td>\n",
       "      <td>0.062050</td>\n",
       "      <td>0.059775</td>\n",
       "      <td>0.022163</td>\n",
       "      <td>-0.010749</td>\n",
       "      <td>0.089941</td>\n",
       "      <td>-0.053717</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>-0.026404</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.013745</td>\n",
       "      <td>-0.009221</td>\n",
       "      <td>-0.013890</td>\n",
       "      <td>-0.045560</td>\n",
       "      <td>-0.018265</td>\n",
       "      <td>-0.006509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>-0.186385</td>\n",
       "      <td>-0.100420</td>\n",
       "      <td>0.074612</td>\n",
       "      <td>0.069428</td>\n",
       "      <td>0.019153</td>\n",
       "      <td>-0.011190</td>\n",
       "      <td>0.112470</td>\n",
       "      <td>-0.060208</td>\n",
       "      <td>0.026991</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.021816</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>0.033347</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>-0.018930</td>\n",
       "      <td>-0.002832</td>\n",
       "      <td>0.012126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.137608</td>\n",
       "      <td>-0.079645</td>\n",
       "      <td>0.054923</td>\n",
       "      <td>0.049929</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.072751</td>\n",
       "      <td>-0.046585</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013403</td>\n",
       "      <td>-0.042277</td>\n",
       "      <td>0.038408</td>\n",
       "      <td>0.063280</td>\n",
       "      <td>0.033056</td>\n",
       "      <td>-0.023695</td>\n",
       "      <td>-0.052760</td>\n",
       "      <td>-0.087968</td>\n",
       "      <td>-0.039740</td>\n",
       "      <td>-0.030263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.005637</td>\n",
       "      <td>-0.179852</td>\n",
       "      <td>-0.097126</td>\n",
       "      <td>0.074498</td>\n",
       "      <td>0.067334</td>\n",
       "      <td>0.021855</td>\n",
       "      <td>-0.013841</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>-0.059853</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>-0.028097</td>\n",
       "      <td>-0.001646</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>-0.008279</td>\n",
       "      <td>-0.008072</td>\n",
       "      <td>-0.039719</td>\n",
       "      <td>-0.015118</td>\n",
       "      <td>-0.002070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.004995</td>\n",
       "      <td>-0.159314</td>\n",
       "      <td>-0.082151</td>\n",
       "      <td>0.064529</td>\n",
       "      <td>0.063772</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>-0.017254</td>\n",
       "      <td>0.103314</td>\n",
       "      <td>-0.054453</td>\n",
       "      <td>0.025985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>-0.009858</td>\n",
       "      <td>-0.028317</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>-0.006525</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.005177</td>\n",
       "      <td>0.019327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>-0.006016</td>\n",
       "      <td>-0.159820</td>\n",
       "      <td>-0.087673</td>\n",
       "      <td>0.063355</td>\n",
       "      <td>0.061493</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>-0.009578</td>\n",
       "      <td>0.093405</td>\n",
       "      <td>-0.052412</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.027015</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>-0.007556</td>\n",
       "      <td>-0.008190</td>\n",
       "      <td>-0.039200</td>\n",
       "      <td>-0.014289</td>\n",
       "      <td>-0.001972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7021</td>\n",
       "      <td>-0.007193</td>\n",
       "      <td>-0.155850</td>\n",
       "      <td>-0.086701</td>\n",
       "      <td>0.061205</td>\n",
       "      <td>0.059566</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>-0.008239</td>\n",
       "      <td>0.092551</td>\n",
       "      <td>-0.052765</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>-0.027081</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.041899</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>-0.008080</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.039849</td>\n",
       "      <td>-0.013918</td>\n",
       "      <td>-0.001909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7022</td>\n",
       "      <td>-0.010107</td>\n",
       "      <td>-0.146512</td>\n",
       "      <td>-0.083579</td>\n",
       "      <td>0.058978</td>\n",
       "      <td>0.057682</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>-0.049590</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011750</td>\n",
       "      <td>-0.032732</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>0.048995</td>\n",
       "      <td>0.020899</td>\n",
       "      <td>-0.014102</td>\n",
       "      <td>-0.027992</td>\n",
       "      <td>-0.061593</td>\n",
       "      <td>-0.026597</td>\n",
       "      <td>-0.015714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7023</td>\n",
       "      <td>-0.007071</td>\n",
       "      <td>-0.144043</td>\n",
       "      <td>-0.079855</td>\n",
       "      <td>0.057958</td>\n",
       "      <td>0.055704</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>-0.047519</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016278</td>\n",
       "      <td>-0.038178</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.052364</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>-0.019552</td>\n",
       "      <td>-0.038423</td>\n",
       "      <td>-0.073152</td>\n",
       "      <td>-0.032518</td>\n",
       "      <td>-0.022682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7024</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>-0.041755</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.031302</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>-0.016998</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032765</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.019534</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>-0.003051</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.011935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7025 rows  300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.009780 -0.153519 -0.086488  0.062050  0.059775  0.022163 -0.010749   \n",
       "1    -0.006704 -0.186385 -0.100420  0.074612  0.069428  0.019153 -0.011190   \n",
       "2    -0.014735 -0.137608 -0.079645  0.054923  0.049929  0.020018 -0.007845   \n",
       "3    -0.005637 -0.179852 -0.097126  0.074498  0.067334  0.021855 -0.013841   \n",
       "4    -0.004995 -0.159314 -0.082151  0.064529  0.063772  0.020675 -0.017254   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7020 -0.006016 -0.159820 -0.087673  0.063355  0.061493  0.019642 -0.009578   \n",
       "7021 -0.007193 -0.155850 -0.086701  0.061205  0.059566  0.018958 -0.008239   \n",
       "7022 -0.010107 -0.146512 -0.083579  0.058978  0.057682  0.021916 -0.009674   \n",
       "7023 -0.007071 -0.144043 -0.079855  0.057958  0.055704  0.021280 -0.011102   \n",
       "7024  0.005002 -0.041755 -0.001683  0.006120  0.031302  0.011992 -0.016998   \n",
       "\n",
       "           7         8         9    ...       290       291       292  \\\n",
       "0     0.089941 -0.053717  0.021990  ... -0.007670 -0.026404  0.004354   \n",
       "1     0.112470 -0.060208  0.026991  ...  0.000254 -0.021816 -0.016526   \n",
       "2     0.072751 -0.046585  0.016632  ... -0.013403 -0.042277  0.038408   \n",
       "3     0.101287 -0.059853  0.024283  ... -0.006704 -0.028097 -0.001646   \n",
       "4     0.103314 -0.054453  0.025985  ... -0.004544 -0.009858 -0.028317   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7020  0.093405 -0.052412  0.023197  ... -0.006700 -0.027015 -0.000707   \n",
       "7021  0.092551 -0.052765  0.022502  ... -0.004267 -0.027081  0.001552   \n",
       "7022  0.084256 -0.049590  0.021110  ... -0.011750 -0.032732  0.015828   \n",
       "7023  0.075940 -0.047519  0.019985  ... -0.016278 -0.038178  0.023187   \n",
       "7024  0.058780 -0.000466  0.012808  ... -0.032765 -0.002467 -0.019534   \n",
       "\n",
       "           293       294       295       296       297       298       299  \n",
       "0     0.041168  0.013745 -0.009221 -0.013890 -0.045560 -0.018265 -0.006509  \n",
       "1     0.033347 -0.001565  0.000954  0.009685 -0.018930 -0.002832  0.012126  \n",
       "2     0.063280  0.033056 -0.023695 -0.052760 -0.087968 -0.039740 -0.030263  \n",
       "3     0.039481  0.007988 -0.008279 -0.008072 -0.039719 -0.015118 -0.002070  \n",
       "4     0.015599 -0.006525  0.005752  0.022966  0.001285  0.005177  0.019327  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7020  0.040323  0.009394 -0.007556 -0.008190 -0.039200 -0.014289 -0.001972  \n",
       "7021  0.041899  0.010162 -0.008080 -0.009752 -0.039849 -0.013918 -0.001909  \n",
       "7022  0.048995  0.020899 -0.014102 -0.027992 -0.061593 -0.026597 -0.015714  \n",
       "7023  0.052364  0.025306 -0.019552 -0.038423 -0.073152 -0.032518 -0.022682  \n",
       "7024  0.006022  0.003704 -0.003051  0.022616  0.006397  0.003268  0.011935  \n",
       "\n",
       "[7025 rows x 300 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDataVecs1=pd.read_csv(\"trainDataVecs1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame()\n",
    "result=pd.concat([trainDataVecs1,cnew['top_class']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"finaloutput.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result=pd.read_csv(\"finaloutput.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=result.iloc[:,0:299]\n",
    "y=pd.DataFrame(result['top_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42,kind='regular',k_neighbors=2)\n",
    "#X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y=label_encoder.fit_transform(y) \n",
    "#X_res, y_res = sm.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.009780</td>\n",
       "      <td>-0.153519</td>\n",
       "      <td>-0.086488</td>\n",
       "      <td>0.062050</td>\n",
       "      <td>0.059775</td>\n",
       "      <td>0.022163</td>\n",
       "      <td>-0.010749</td>\n",
       "      <td>0.089941</td>\n",
       "      <td>-0.053717</td>\n",
       "      <td>0.021990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017769</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>-0.026404</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.041168</td>\n",
       "      <td>0.013745</td>\n",
       "      <td>-0.009221</td>\n",
       "      <td>-0.013890</td>\n",
       "      <td>-0.045560</td>\n",
       "      <td>-0.018265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>-0.186385</td>\n",
       "      <td>-0.100420</td>\n",
       "      <td>0.074612</td>\n",
       "      <td>0.069428</td>\n",
       "      <td>0.019153</td>\n",
       "      <td>-0.011190</td>\n",
       "      <td>0.112470</td>\n",
       "      <td>-0.060208</td>\n",
       "      <td>0.026991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013348</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>-0.021816</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>0.033347</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.009685</td>\n",
       "      <td>-0.018930</td>\n",
       "      <td>-0.002832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.014735</td>\n",
       "      <td>-0.137608</td>\n",
       "      <td>-0.079645</td>\n",
       "      <td>0.054923</td>\n",
       "      <td>0.049929</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.072751</td>\n",
       "      <td>-0.046585</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027914</td>\n",
       "      <td>-0.013403</td>\n",
       "      <td>-0.042277</td>\n",
       "      <td>0.038408</td>\n",
       "      <td>0.063280</td>\n",
       "      <td>0.033056</td>\n",
       "      <td>-0.023695</td>\n",
       "      <td>-0.052760</td>\n",
       "      <td>-0.087968</td>\n",
       "      <td>-0.039740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.005637</td>\n",
       "      <td>-0.179852</td>\n",
       "      <td>-0.097126</td>\n",
       "      <td>0.074498</td>\n",
       "      <td>0.067334</td>\n",
       "      <td>0.021855</td>\n",
       "      <td>-0.013841</td>\n",
       "      <td>0.101287</td>\n",
       "      <td>-0.059853</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015320</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>-0.028097</td>\n",
       "      <td>-0.001646</td>\n",
       "      <td>0.039481</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>-0.008279</td>\n",
       "      <td>-0.008072</td>\n",
       "      <td>-0.039719</td>\n",
       "      <td>-0.015118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.004995</td>\n",
       "      <td>-0.159314</td>\n",
       "      <td>-0.082151</td>\n",
       "      <td>0.064529</td>\n",
       "      <td>0.063772</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>-0.017254</td>\n",
       "      <td>0.103314</td>\n",
       "      <td>-0.054453</td>\n",
       "      <td>0.025985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004213</td>\n",
       "      <td>-0.004544</td>\n",
       "      <td>-0.009858</td>\n",
       "      <td>-0.028317</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>-0.006525</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.022966</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.005177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>-0.006016</td>\n",
       "      <td>-0.159820</td>\n",
       "      <td>-0.087673</td>\n",
       "      <td>0.063355</td>\n",
       "      <td>0.061493</td>\n",
       "      <td>0.019642</td>\n",
       "      <td>-0.009578</td>\n",
       "      <td>0.093405</td>\n",
       "      <td>-0.052412</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016774</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.027015</td>\n",
       "      <td>-0.000707</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>-0.007556</td>\n",
       "      <td>-0.008190</td>\n",
       "      <td>-0.039200</td>\n",
       "      <td>-0.014289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7021</td>\n",
       "      <td>-0.007193</td>\n",
       "      <td>-0.155850</td>\n",
       "      <td>-0.086701</td>\n",
       "      <td>0.061205</td>\n",
       "      <td>0.059566</td>\n",
       "      <td>0.018958</td>\n",
       "      <td>-0.008239</td>\n",
       "      <td>0.092551</td>\n",
       "      <td>-0.052765</td>\n",
       "      <td>0.022502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017100</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>-0.027081</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.041899</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>-0.008080</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>-0.039849</td>\n",
       "      <td>-0.013918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7022</td>\n",
       "      <td>-0.010107</td>\n",
       "      <td>-0.146512</td>\n",
       "      <td>-0.083579</td>\n",
       "      <td>0.058978</td>\n",
       "      <td>0.057682</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>0.084256</td>\n",
       "      <td>-0.049590</td>\n",
       "      <td>0.021110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021768</td>\n",
       "      <td>-0.011750</td>\n",
       "      <td>-0.032732</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>0.048995</td>\n",
       "      <td>0.020899</td>\n",
       "      <td>-0.014102</td>\n",
       "      <td>-0.027992</td>\n",
       "      <td>-0.061593</td>\n",
       "      <td>-0.026597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7023</td>\n",
       "      <td>-0.007071</td>\n",
       "      <td>-0.144043</td>\n",
       "      <td>-0.079855</td>\n",
       "      <td>0.057958</td>\n",
       "      <td>0.055704</td>\n",
       "      <td>0.021280</td>\n",
       "      <td>-0.011102</td>\n",
       "      <td>0.075940</td>\n",
       "      <td>-0.047519</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021990</td>\n",
       "      <td>-0.016278</td>\n",
       "      <td>-0.038178</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.052364</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>-0.019552</td>\n",
       "      <td>-0.038423</td>\n",
       "      <td>-0.073152</td>\n",
       "      <td>-0.032518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7024</td>\n",
       "      <td>0.005002</td>\n",
       "      <td>-0.041755</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.031302</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>-0.016998</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>-0.032765</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.019534</td>\n",
       "      <td>0.006022</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>-0.003051</td>\n",
       "      <td>0.022616</td>\n",
       "      <td>0.006397</td>\n",
       "      <td>0.003268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7025 rows  299 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.009780 -0.153519 -0.086488  0.062050  0.059775  0.022163 -0.010749   \n",
       "1    -0.006704 -0.186385 -0.100420  0.074612  0.069428  0.019153 -0.011190   \n",
       "2    -0.014735 -0.137608 -0.079645  0.054923  0.049929  0.020018 -0.007845   \n",
       "3    -0.005637 -0.179852 -0.097126  0.074498  0.067334  0.021855 -0.013841   \n",
       "4    -0.004995 -0.159314 -0.082151  0.064529  0.063772  0.020675 -0.017254   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7020 -0.006016 -0.159820 -0.087673  0.063355  0.061493  0.019642 -0.009578   \n",
       "7021 -0.007193 -0.155850 -0.086701  0.061205  0.059566  0.018958 -0.008239   \n",
       "7022 -0.010107 -0.146512 -0.083579  0.058978  0.057682  0.021916 -0.009674   \n",
       "7023 -0.007071 -0.144043 -0.079855  0.057958  0.055704  0.021280 -0.011102   \n",
       "7024  0.005002 -0.041755 -0.001683  0.006120  0.031302  0.011992 -0.016998   \n",
       "\n",
       "             7         8         9  ...       289       290       291  \\\n",
       "0     0.089941 -0.053717  0.021990  ... -0.017769 -0.007670 -0.026404   \n",
       "1     0.112470 -0.060208  0.026991  ... -0.013348  0.000254 -0.021816   \n",
       "2     0.072751 -0.046585  0.016632  ... -0.027914 -0.013403 -0.042277   \n",
       "3     0.101287 -0.059853  0.024283  ... -0.015320 -0.006704 -0.028097   \n",
       "4     0.103314 -0.054453  0.025985  ... -0.004213 -0.004544 -0.009858   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7020  0.093405 -0.052412  0.023197  ... -0.016774 -0.006700 -0.027015   \n",
       "7021  0.092551 -0.052765  0.022502  ... -0.017100 -0.004267 -0.027081   \n",
       "7022  0.084256 -0.049590  0.021110  ... -0.021768 -0.011750 -0.032732   \n",
       "7023  0.075940 -0.047519  0.019985  ... -0.021990 -0.016278 -0.038178   \n",
       "7024  0.058780 -0.000466  0.012808  ...  0.004613 -0.032765 -0.002467   \n",
       "\n",
       "           292       293       294       295       296       297       298  \n",
       "0     0.004354  0.041168  0.013745 -0.009221 -0.013890 -0.045560 -0.018265  \n",
       "1    -0.016526  0.033347 -0.001565  0.000954  0.009685 -0.018930 -0.002832  \n",
       "2     0.038408  0.063280  0.033056 -0.023695 -0.052760 -0.087968 -0.039740  \n",
       "3    -0.001646  0.039481  0.007988 -0.008279 -0.008072 -0.039719 -0.015118  \n",
       "4    -0.028317  0.015599 -0.006525  0.005752  0.022966  0.001285  0.005177  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7020 -0.000707  0.040323  0.009394 -0.007556 -0.008190 -0.039200 -0.014289  \n",
       "7021  0.001552  0.041899  0.010162 -0.008080 -0.009752 -0.039849 -0.013918  \n",
       "7022  0.015828  0.048995  0.020899 -0.014102 -0.027992 -0.061593 -0.026597  \n",
       "7023  0.023187  0.052364  0.025306 -0.019552 -0.038423 -0.073152 -0.032518  \n",
       "7024 -0.019534  0.006022  0.003704 -0.003051  0.022616  0.006397  0.003268  \n",
       "\n",
       "[7025 rows x 299 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res, y_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_res, y_res,test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9262, 299), (9262,), (4562, 299), (4562,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_res.shape,y_train_res.shape,X_test_res.shape,y_test_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf=RandomForestClassifier(n_estimators=400,max_depth=8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = { \n",
    "    'n_estimators': [100,200,200,400,500],\n",
    "    #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [3,4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=RandomForestClassifier(max_depth=7, n_estimators=200,\n",
       "                                              random_state=0),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [3, 4, 5, 6, 7, 8],\n",
       "                         'n_estimators': [100, 200, 200, 400, 500]})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 2)\n",
    "CV_rfc.fit(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 400}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=8, n_estimators=400, random_state=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9856402504858562"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9787373958790004"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test_res, y_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1367</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2\n",
       "0  1528     0     0\n",
       "1     5  1367    88\n",
       "2     0     4  1570"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_test_res, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randor_forest_classification report                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1528\n",
      "           1       1.00      0.94      0.97      1460\n",
      "           2       0.95      1.00      0.97      1574\n",
      "\n",
      "    accuracy                           0.98      4562\n",
      "   macro avg       0.98      0.98      0.98      4562\n",
      "weighted avg       0.98      0.98      0.98      4562\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\"Randor_forest_classification report \",classification_report(y_test_res, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789337919174549"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9771186440677966"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2\n",
       "0  0     2  0\n",
       "1  0  2277  0\n",
       "2  0    47  0"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
