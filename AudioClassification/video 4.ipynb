{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\Anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import glob\n",
    "import moviepy.editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(r\"C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video4\\*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "chunk:   0%|                                                                        | 0/5468 [00:00<?, ?it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video4\\video_4.mp4\n",
      "MoviePy - Writing audio in C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video4\\video_4.mp4_new_audio_.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for i in files:\n",
    "    video=moviepy.editor.VideoFileClip(i)\n",
    "    audio=video.audio\n",
    "    print(i)\n",
    "    path= i+\"_\"+'new_audio_.wav'\n",
    "    audio.write_audiofile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = glob.glob(r\"C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video4\\*.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\pradeep\\\\text\\\\UPDATE ON 8-DEC-2020\\\\validation\\\\csv file\\\\files\\\\video4\\\\video_4.mp4_new_audio_.wav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class SplitWavAudioMubin():\n",
    "    def __init__(self, folder, filename):\n",
    "        self.folder = folder\n",
    "        self.filename = filename\n",
    "        self.filepath = folder + '\\\\' + filename\n",
    "        \n",
    "        self.audio = AudioSegment.from_wav(self.filepath)\n",
    "    \n",
    "    def get_duration(self):\n",
    "        return self.audio.duration_seconds\n",
    "    \n",
    "    def single_split(self, from_min, to_min, split_filename):\n",
    "        t1 = from_min * 60 * 1000\n",
    "        t2 = to_min * 60 * 1000\n",
    "        split_audio = self.audio[t1:t2]\n",
    "        split_audio.export(self.folder + '\\\\' + split_filename, format=\"wav\")\n",
    "        \n",
    "    def multiple_split(self, min_per_split):\n",
    "        total_mins = math.ceil(self.get_duration() / 60)\n",
    "        for i in range(0, total_mins, min_per_split):\n",
    "            split_fn = str(i) + '_' + self.filename\n",
    "            self.single_split(i, i+min_per_split, split_fn)\n",
    "            print(str(i) + ' Done')\n",
    "            if i == total_mins - min_per_split:\n",
    "                print('All splited successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video4\\video_4.mp4_new_audio_.wav\n"
     ]
    }
   ],
   "source": [
    "for i in files1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = r'C:\\Users\\PRAVEEN\\Desktop\\Hate Speech\\videos'\n",
    "#for i in files1:\n",
    "    #file=i\n",
    "    #split_wav = SplitWavAudioMubin(folder, i)\n",
    "    #split_wav.multiple_split(min_per_split=1)\n",
    "#file = 'videoplayback (1).mp4_new_audio_.wav'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr \n",
    "import os \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# create a speech recognition object\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# a function that splits the audio file into chunks\n",
    "# and applies speech recognition\n",
    "aud1=[]\n",
    "def get_large_audio_transcription(path):\n",
    "    \"\"\"\n",
    "    Splitting the large audio file into chunks\n",
    "    and apply speech recognition on each of these chunks\n",
    "    \"\"\"\n",
    "    # open the audio file using pydub\n",
    "    sound = AudioSegment.from_wav(path)  \n",
    "    # split audio sound where silence is 700 miliseconds or more and get chunks\n",
    "    chunks = split_on_silence(sound,\n",
    "        # experiment with this value for your target audio file\n",
    "        min_silence_len = 500,\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = sound.dBFS-14,\n",
    "        # keep the silence for 1 second, adjustable as well\n",
    "        keep_silence=500,\n",
    "    )\n",
    "    folder_name = \"audio-chunks\"\n",
    "    # create a directory to store the audio chunks\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "    whole_text = \"\"\n",
    "    # process each chunk \n",
    "    for i, audio_chunk in enumerate(chunks, start=1):\n",
    "        # export audio chunk and save it in\n",
    "        # the `folder_name` directory.\n",
    "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
    "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(chunk_filename) as source:\n",
    "            audio_listened = r.record(source)\n",
    "            # try converting it to text\n",
    "            try:\n",
    "                text = r.recognize_google(audio_listened)\n",
    "            except sr.UnknownValueError as e:\n",
    "                print(\"Error:\", str(e))\n",
    "            else:\n",
    "                text = f\"{text.capitalize()}. \"\n",
    "                print(chunk_filename, \":\", text)\n",
    "                whole_text += text\n",
    "                #aud1.append(whole_text)\n",
    "    aud1.append(whole_text)           \n",
    "    # return the text for all chunks detected\n",
    "    return whole_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\video4\\video_4.mp4_new_audio_.wav\n",
      "audio-chunks\\chunk1.wav : So what i found out there is a sector but actually i have had to go. \n",
      "audio-chunks\\chunk2.wav : I want a sector 12 factory question it is impossible. \n",
      "audio-chunks\\chunk3.wav : I am delighted to report back. \n",
      "audio-chunks\\chunk4.wav : The answer is no. \n",
      "audio-chunks\\chunk5.wav : Only till half of the dolls awarded have p**** hair. \n",
      "audio-chunks\\chunk6.wav : Customers can an extra $1400 for it i spent $1,400 getting my in laser offices proof. \n",
      "audio-chunks\\chunk7.wav : We are not communicate with each other. \n",
      "audio-chunks\\chunk8.wav : I want to talk with you doing with robot to achieve right i want to see the boobs is my hair everyday what's the same time table. \n",
      "audio-chunks\\chunk9.wav : No no. \n",
      "audio-chunks\\chunk10.wav : Is awarded naples 38 different times one of the most popular what are the four great news ladies rubber shape unipolarism. \n",
      "audio-chunks\\chunk11.wav : Distracted and the guy who put the poles on the robber tribe. \n",
      "audio-chunks\\chunk12.wav : 24 hours open hair. \n",
      "audio-chunks\\chunk13.wav : Take that ryan gosling space. \n",
      "audio-chunks\\chunk14.wav : The great news for me and you as my no wife. \n",
      "audio-chunks\\chunk15.wav : Eating disorders and so i was in my life. \n",
      "audio-chunks\\chunk16.wav : Analysis of my boobs student different sizes side effects of eating disorder sometimes do boobs. \n",
      "audio-chunks\\chunk17.wav : One is bigger than other one. \n",
      "audio-chunks\\chunk18.wav : The same general fare one was just like a schedule version of the other one is like alec baldwin and stephen baldwin to stevens you know. \n",
      "audio-chunks\\chunk19.wav : Virtual access points. \n",
      "audio-chunks\\chunk20.wav : And i will be soon be arrested for it i had to explain why he was able to see and hear him not to like to you like bass. \n",
      "audio-chunks\\chunk21.wav : Young women. \n",
      "audio-chunks\\chunk22.wav : You don't have to the tragic irony of this whole thing is that that's not even because one of the surgeons made a mistake in 1900. \n",
      "audio-chunks\\chunk23.wav : I know i never get an operation by the airports. \n",
      "audio-chunks\\chunk24.wav : And i hate them for the last couple years. \n",
      "audio-chunks\\chunk25.wav : I hate them because i thought a shame my whole thing is you know it's up to you are love you so i was having all the time. \n",
      "audio-chunks\\chunk26.wav : Right down to the surgeon in his i can understand this entry makes no sense it's all time slip of protons never take right to take a new only broker shoulder had this happen. \n",
      "audio-chunks\\chunk27.wav : Mum and i realise that i found myself and my pics itself my life. \n"
     ]
    }
   ],
   "source": [
    "for i in files1:\n",
    "    print(i)\n",
    "    get_large_audio_transcription(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "aud1=pd.DataFrame(aud1)\n",
    "aud1.replace(\"[^a-zA-Z.]\",\" \",regex=True,inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "aud1.replace(\"\", nan_value, inplace=True)\n",
    "aud1.dropna(axis=0,how='any',thresh=None,subset=None,inplace=True)\n",
    "data=aud1\n",
    "#len(data)\n",
    "headline=[]\n",
    "for row in range(0,len(data.index)):\n",
    "    headline.append(''.join(str(x) for x in data.iloc[row]))\n",
    "headline=str(headline)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline.lower()\n",
    "sentence=headline.split(\".\")\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "tokenText = []\n",
    "for sent in sentence:\n",
    "    tok = nltk.word_tokenize(sent)\n",
    "    if len(tok) > 0:\n",
    "        tokenText.append(tok)      \n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english')) \n",
    "filtered_sentence_new = [] \n",
    "filtered_sentence_new = [word for word in tokenText if not word in stopwords.words()]\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "def wordlema(text):\n",
    "    lem_text=\" \".join([lemmatizer.lemmatize(i) for i in  text])\n",
    "    return lem_text\n",
    "lemtxt1 = []\n",
    "for tok in filtered_sentence_new:\n",
    "    lemSent = wordlema(tok)\n",
    "    lemtxt1.append(lemSent)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud2=pd.DataFrame(lemtxt1)\n",
    "aud2.replace(\"[^a-zA-Z.]\",\" \",regex=True,inplace=True)\n",
    "nan_value = float(\"NaN\")\n",
    "aud2.replace(\"\", nan_value, inplace=True)\n",
    "aud2.dropna(axis=0,how='any',thresh=None,subset=None,inplace=True)\n",
    "data2=aud2\n",
    "#len(data)\n",
    "headline_new=[]\n",
    "for row in range(0,len(data2.index)):\n",
    "    headline_new.append(''.join(str(x) for x in data2.iloc[row]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   So what i found out there is a sector but actually i have had to go',\n",
       " 'I want a sector factory question it is impossible',\n",
       " 'I am delighted to report back',\n",
       " 'The answer is no',\n",
       " 'Only till half of the doll awarded have p hair',\n",
       " 'Customers can an extra for it i spent getting my in laser office proof',\n",
       " 'We are not communicate with each other',\n",
       " 'I want to talk with you doing with robot to achieve right i want to see the boob is my hair everyday what s the same time table',\n",
       " 'No no',\n",
       " 'Is awarded naples different time one of the most popular what are the four great news lady rubber shape unipolarism',\n",
       " 'Distracted and the guy who put the pole on the robber tribe',\n",
       " 'hour open hair',\n",
       " 'Take that ryan gosling space',\n",
       " 'The great news for me and you a my no wife',\n",
       " 'Eating disorder and so i wa in my life',\n",
       " 'Analysis of my boob student different size side effect of eating disorder sometimes do boob',\n",
       " 'One is bigger than other one',\n",
       " 'The same general fare one wa just like a schedule version of the other one is like alec baldwin and stephen baldwin to stevens you know',\n",
       " 'Virtual access point',\n",
       " 'And i will be soon be arrested for it i had to explain why he wa able to see and hear him not to like to you like bass',\n",
       " 'Young woman',\n",
       " 'You don t have to the tragic irony of this whole thing is that that s not even because one of the surgeon made a mistake in',\n",
       " 'I know i never get an operation by the airport',\n",
       " 'And i hate them for the last couple year',\n",
       " 'I hate them because i thought a shame my whole thing is you know it s up to you are love you so i wa having all the time',\n",
       " 'Right down to the surgeon in his i can understand this entry make no sense it s all time slip of proton never take right to take a new only broker shoulder had this happen',\n",
       " 'Mum and i realise that i found myself and my pic itself my life',\n",
       " '   ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline=pd.DataFrame(headline_new)\n",
    "headline.to_csv('video4.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model=gensim.models.Word2Vec(headline_new,workers=3,size=100,min_count=40,window=10,sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words):\n",
    "  #for sent in list_of_sent:\n",
    "  feature_vec=np.zeros((100,),dtype='float32')\n",
    "  nwords = 0\n",
    "  index2word_set= set(w2v_model.wv.index2word) \n",
    "  for word in words:\n",
    "    if word in index2word_set:\n",
    "        nwords +=1\n",
    "        feature_vec=np.add(feature_vec,w2v_model[word])     \n",
    "  feature_vec=np.divide(feature_vec,nwords)\n",
    "  feature_vec = np.around(feature_vec,3)\n",
    "  return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_feature_vec(tweets):\n",
    "    c=0\n",
    "    tweet_feature_vec = np.zeros((len(tweets),100),dtype='float32')\n",
    "    for tweet in tweets:\n",
    "      tweet_feature_vec[c]=make_feature_vec(tweet)\n",
    "      c=c+1\n",
    "    return tweet_feature_vec\n",
    "#tweets=[]\n",
    "#for tweet in df['tweet']:\n",
    "  #tweets.append(tweet)\n",
    "x1= get_avg_feature_vec(headline_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1=pd.DataFrame(x1)\n",
    "x1.fillna(x1.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "gboost_from_joblib = joblib.load(r'C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\word2vecsamp.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "gboost_from_joblib.predict(x1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 730)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,3),max_features=10000)\n",
    "tfidf.fit(headline_new)\n",
    "headline_new1= tfidf.transform(headline_new)\n",
    "headline_new1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'hate', 'noHate',\n",
       "       'hate', 'noHate', 'hate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_from_joblib = joblib.load(r'C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\tfidfsamp730.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "svm_from_joblib.predict(headline_new1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "window_size = 40\n",
    "min_word = 5\n",
    "down_sampling = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(headline_new,\n",
    "                      size=embedding_size,\n",
    "                      window=window_size,\n",
    "                      min_count=min_word,\n",
    "                      sample=down_sampling,\n",
    "                      sg=1,\n",
    "                      iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words):\n",
    "  #for sent in list_of_sent:\n",
    "  feature_vec=np.zeros((100,),dtype='float32')\n",
    "  nwords = 0\n",
    "  index2word_set= set(ft_model.wv.index2word) \n",
    "  for word in words:\n",
    "    if word in index2word_set:\n",
    "        nwords +=1\n",
    "        feature_vec=np.add(feature_vec,ft_model[word])     \n",
    "  feature_vec=np.divide(feature_vec,nwords)\n",
    "  feature_vec = np.around(feature_vec,3)\n",
    "  return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_feature_vec(tweets):\n",
    "    c=0\n",
    "    tweet_feature_vec = np.zeros((len(tweets),100),dtype='float32')\n",
    "    for tweet in tweets:\n",
    "      tweet_feature_vec[c]=make_feature_vec(tweet)\n",
    "      c=c+1\n",
    "    return tweet_feature_vec\n",
    "#tweets=[]\n",
    "#for tweet in df['tweet']:\n",
    "  #tweets.append(tweet)\n",
    "x= get_avg_feature_vec(headline_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate', 'noHate', 'noHate',\n",
       "       'noHate', 'noHate', 'noHate', 'noHate'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientboost_from_joblib = joblib.load(r'C:\\Users\\pradeep\\text\\UPDATE ON 8-DEC-2020\\validation\\csv file\\files\\fasttextsamp.pkl')  \n",
    "  \n",
    "# Use the loaded model to make predictions \n",
    "gradientboost_from_joblib.predict(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>So what i found out there is a sector but a...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I want a sector factory question it is impossible</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>I am delighted to report back</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>The answer is no</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Only till half of the doll awarded have p hair</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Customers can an extra for it i spent getting ...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>We are not communicate with each other</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>I want to talk with you doing with robot to ac...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>No no</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Is awarded naples different time one of the mo...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Distracted and the guy who put the pole on the...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>hour open hair</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Take that ryan gosling space</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>The great news for me and you a my no wife</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>Eating disorder and so i wa in my life</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Analysis of my boob student different size sid...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>One is bigger than other one</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>The same general fare one wa just like a sched...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>Virtual access point</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>And i will be soon be arrested for it i had to...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Young woman</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>You don t have to the tragic irony of this who...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>I know i never get an operation by the airport</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>And i hate them for the last couple year</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>I hate them because i thought a shame my whole...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>Right down to the surgeon in his i can underst...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>Mum and i realise that i found myself and my p...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                                  0 Unnamed: 2\n",
       "0            0     So what i found out there is a sector but a...     noHate\n",
       "1            1  I want a sector factory question it is impossible     noHate\n",
       "2            2                      I am delighted to report back     noHate\n",
       "3            3                                   The answer is no     noHate\n",
       "4            4     Only till half of the doll awarded have p hair     noHate\n",
       "5            5  Customers can an extra for it i spent getting ...     noHate\n",
       "6            6             We are not communicate with each other     noHate\n",
       "7            7  I want to talk with you doing with robot to ac...       hate\n",
       "8            8                                              No no     noHate\n",
       "9            9  Is awarded naples different time one of the mo...     noHate\n",
       "10          10  Distracted and the guy who put the pole on the...     noHate\n",
       "11          11                                     hour open hair     noHate\n",
       "12          12                       Take that ryan gosling space     noHate\n",
       "13          13         The great news for me and you a my no wife     noHate\n",
       "14          14             Eating disorder and so i wa in my life     noHate\n",
       "15          15  Analysis of my boob student different size sid...       hate\n",
       "16          16                       One is bigger than other one     noHate\n",
       "17          17  The same general fare one wa just like a sched...       hate\n",
       "18          18                               Virtual access point     noHate\n",
       "19          19  And i will be soon be arrested for it i had to...       hate\n",
       "20          20                                        Young woman     noHate\n",
       "21          21  You don t have to the tragic irony of this who...       hate\n",
       "22          22     I know i never get an operation by the airport     noHate\n",
       "23          23           And i hate them for the last couple year     noHate\n",
       "24          24  I hate them because i thought a shame my whole...     noHate\n",
       "25          25  Right down to the surgeon in his i can underst...     noHate\n",
       "26          26  Mum and i realise that i found myself and my p...     noHate"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline1=pd.read_csv('video4.csv')\n",
    "headline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline1=headline1.iloc[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So what i found out there is a sector but a...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I want a sector factory question it is impossible</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I am delighted to report back</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The answer is no</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Only till half of the doll awarded have p hair</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Customers can an extra for it i spent getting ...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>We are not communicate with each other</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>I want to talk with you doing with robot to ac...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No no</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Is awarded naples different time one of the mo...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Distracted and the guy who put the pole on the...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>hour open hair</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Take that ryan gosling space</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>The great news for me and you a my no wife</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Eating disorder and so i wa in my life</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Analysis of my boob student different size sid...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>One is bigger than other one</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>The same general fare one wa just like a sched...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Virtual access point</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>And i will be soon be arrested for it i had to...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Young woman</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>You don t have to the tragic irony of this who...</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>I know i never get an operation by the airport</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>And i hate them for the last couple year</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>I hate them because i thought a shame my whole...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Right down to the surgeon in his i can underst...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Mum and i realise that i found myself and my p...</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0 Unnamed: 2\n",
       "0      So what i found out there is a sector but a...     noHate\n",
       "1   I want a sector factory question it is impossible     noHate\n",
       "2                       I am delighted to report back     noHate\n",
       "3                                    The answer is no     noHate\n",
       "4      Only till half of the doll awarded have p hair     noHate\n",
       "5   Customers can an extra for it i spent getting ...     noHate\n",
       "6              We are not communicate with each other     noHate\n",
       "7   I want to talk with you doing with robot to ac...       hate\n",
       "8                                               No no     noHate\n",
       "9   Is awarded naples different time one of the mo...     noHate\n",
       "10  Distracted and the guy who put the pole on the...     noHate\n",
       "11                                     hour open hair     noHate\n",
       "12                       Take that ryan gosling space     noHate\n",
       "13         The great news for me and you a my no wife     noHate\n",
       "14             Eating disorder and so i wa in my life     noHate\n",
       "15  Analysis of my boob student different size sid...       hate\n",
       "16                       One is bigger than other one     noHate\n",
       "17  The same general fare one wa just like a sched...       hate\n",
       "18                               Virtual access point     noHate\n",
       "19  And i will be soon be arrested for it i had to...       hate\n",
       "20                                        Young woman     noHate\n",
       "21  You don t have to the tragic irony of this who...       hate\n",
       "22     I know i never get an operation by the airport     noHate\n",
       "23           And i hate them for the last couple year     noHate\n",
       "24  I hate them because i thought a shame my whole...     noHate\n",
       "25  Right down to the surgeon in his i can underst...     noHate\n",
       "26  Mum and i realise that i found myself and my p...     noHate"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>FastText</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>Word2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>So what i found out there is a sector but a...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I want a sector factory question it is impossible</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I am delighted to report back</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>The answer is no</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Only till half of the doll awarded have p hair</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Customers can an extra for it i spent getting ...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>We are not communicate with each other</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>I want to talk with you doing with robot to ac...</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No no</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Is awarded naples different time one of the mo...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Distracted and the guy who put the pole on the...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>hour open hair</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Take that ryan gosling space</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>The great news for me and you a my no wife</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Eating disorder and so i wa in my life</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Analysis of my boob student different size sid...</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>One is bigger than other one</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>The same general fare one wa just like a sched...</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Virtual access point</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>And i will be soon be arrested for it i had to...</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Young woman</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>You don t have to the tragic irony of this who...</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>hate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>I know i never get an operation by the airport</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>And i hate them for the last couple year</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>I hate them because i thought a shame my whole...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Right down to the surgeon in his i can underst...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Mum and i realise that i found myself and my p...</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0 Unnamed: 2 FastText  \\\n",
       "0      So what i found out there is a sector but a...     noHate   noHate   \n",
       "1   I want a sector factory question it is impossible     noHate   noHate   \n",
       "2                       I am delighted to report back     noHate   noHate   \n",
       "3                                    The answer is no     noHate   noHate   \n",
       "4      Only till half of the doll awarded have p hair     noHate   noHate   \n",
       "5   Customers can an extra for it i spent getting ...     noHate   noHate   \n",
       "6              We are not communicate with each other     noHate   noHate   \n",
       "7   I want to talk with you doing with robot to ac...       hate   noHate   \n",
       "8                                               No no     noHate   noHate   \n",
       "9   Is awarded naples different time one of the mo...     noHate   noHate   \n",
       "10  Distracted and the guy who put the pole on the...     noHate   noHate   \n",
       "11                                     hour open hair     noHate   noHate   \n",
       "12                       Take that ryan gosling space     noHate   noHate   \n",
       "13         The great news for me and you a my no wife     noHate   noHate   \n",
       "14             Eating disorder and so i wa in my life     noHate   noHate   \n",
       "15  Analysis of my boob student different size sid...       hate   noHate   \n",
       "16                       One is bigger than other one     noHate   noHate   \n",
       "17  The same general fare one wa just like a sched...       hate   noHate   \n",
       "18                               Virtual access point     noHate   noHate   \n",
       "19  And i will be soon be arrested for it i had to...       hate   noHate   \n",
       "20                                        Young woman     noHate   noHate   \n",
       "21  You don t have to the tragic irony of this who...       hate   noHate   \n",
       "22     I know i never get an operation by the airport     noHate   noHate   \n",
       "23           And i hate them for the last couple year     noHate   noHate   \n",
       "24  I hate them because i thought a shame my whole...     noHate   noHate   \n",
       "25  Right down to the surgeon in his i can underst...     noHate   noHate   \n",
       "26  Mum and i realise that i found myself and my p...     noHate   noHate   \n",
       "\n",
       "     TFIDF Word2Vec  \n",
       "0   noHate   noHate  \n",
       "1   noHate   noHate  \n",
       "2   noHate   noHate  \n",
       "3   noHate   noHate  \n",
       "4   noHate   noHate  \n",
       "5   noHate   noHate  \n",
       "6   noHate   noHate  \n",
       "7   noHate   noHate  \n",
       "8   noHate   noHate  \n",
       "9   noHate   noHate  \n",
       "10  noHate   noHate  \n",
       "11  noHate   noHate  \n",
       "12  noHate   noHate  \n",
       "13  noHate   noHate  \n",
       "14  noHate   noHate  \n",
       "15  noHate   noHate  \n",
       "16  noHate   noHate  \n",
       "17    hate   noHate  \n",
       "18  noHate   noHate  \n",
       "19    hate   noHate  \n",
       "20  noHate   noHate  \n",
       "21    hate   noHate  \n",
       "22  noHate   noHate  \n",
       "23  noHate   noHate  \n",
       "24  noHate   noHate  \n",
       "25  noHate   noHate  \n",
       "26  noHate   noHate  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline1['FastText']=pd.DataFrame(gradientboost_from_joblib.predict(x))\n",
    "headline1['TFIDF']=pd.DataFrame(svm_from_joblib.predict(headline_new1))\n",
    "headline1['Word2Vec']=pd.DataFrame(gboost_from_joblib.predict(x1))\n",
    "headline1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1\n",
       "0  0   0\n",
       "1  4  23"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=headline1['Unnamed: 2']\n",
    "pred=headline1['FastText']\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "pd.DataFrame(confusion_matrix(pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1\n",
       "0  3   0\n",
       "1  2  22"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=headline1['Unnamed: 2']\n",
    "pred1=headline1['TFIDF']\n",
    "pd.DataFrame(confusion_matrix(pred1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1\n",
       "0  0   0\n",
       "1  5  22"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=headline1['Unnamed: 2']\n",
    "pred2=headline1['Word2Vec']\n",
    "pd.DataFrame(confusion_matrix(pred2,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
