{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperasOnColab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DdsepW1GU_Z",
        "outputId": "fe610675-ada4-4fe7-8f64-97ca49d275db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRMc7g8kGi38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1685c374-945d-41ce-de13-1179f321bc8d"
      },
      "source": [
        "# Import Library and code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "from keras.models import Model, model_from_json\n",
        "!pip install keras_tuner\n",
        "import kerastuner as kt\n",
        "from numpy import load"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 21.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 15.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 11.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 11.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (20.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.19.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (0.8.9)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras_tuner) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras_tuner) (1.0.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp37-none-any.whl size=78938 sha256=68231b05778cdef6664754753eacf98e8d0444372b407358f50e239ceb89a5a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=bb2b53714f20cb2c0454b1263dc34fa39b0a6e80c815506045425e653d8f8e72\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNtnec8HKWa1"
      },
      "source": [
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding, Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAIAJBVvLy2w"
      },
      "source": [
        "def data():\n",
        "  x_train_load = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/x_train.npy')\n",
        "  y_train_load = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/y_train.npy')\n",
        "  x_test_load = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/x_test.npy')\n",
        "  y_test_load = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/y_test.npy')\n",
        "  return x_train_load, y_train_load, x_test_load, y_test_load"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_jjPJ2yBy-4"
      },
      "source": [
        "x_train, y_train, x_test, y_test = data()\n",
        "# x_train"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OilthnqhBsHi",
        "outputId": "a03f6cfd-040d-4402-853b-e75aac446d9a"
      },
      "source": [
        "len(x_train[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jf-Dzv_B-hG"
      },
      "source": [
        "embedding_matrix = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/embedding_matrix.npy')\n",
        "# embedding_matrix"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbcDaNS9EOcF",
        "outputId": "acaa7723-5ed5-45fb-b227-03554c05c4b2"
      },
      "source": [
        "len(embedding_matrix)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "185296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7E9b6mLPVIp"
      },
      "source": [
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whTPD8YAx7Ik"
      },
      "source": [
        "!pip install hyperas\n",
        "from hyperas import optim\n",
        "from hyperas.distributions import choice, uniform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSCi3TMkzaji"
      },
      "source": [
        "!pip install hyperopt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxuAhivw0Lsl"
      },
      "source": [
        "from hyperopt import Trials, STATUS_OK, tpe\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMrMOAfaLG67"
      },
      "source": [
        "def create_model(x_train, y_train, x_test, y_test):\n",
        "    layer = Embedding(input_dim = 185296,output_dim = 100,weights=[embedding_matrix],input_length=100,trainable=False)\n",
        "    model = Sequential()\n",
        "    model.add(layer)\n",
        "    for i in range(3):\n",
        "      model.add(Bidirectional(LSTM({{choice[50, 100, 150, 200]}}, dropout= {{uniform(0.1, 0.5)}}, return_sequences=True)))\n",
        "    # model.add(Bidirectional(LSTM(128, dropout=0.3, return_sequences=True)))\n",
        "    # model.add(Bidirectional(LSTM(100, dropout=0.1, return_sequences=True)))\n",
        "    model.add(Conv1D(64, 5, activation='relu'))\n",
        "    model.add(GlobalMaxPool1D())\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    result = model.fit(x_train, y_train,batch_size=512,epochs=7,validation_split=0.2,callbacks=callbacks,verbose=1) \n",
        "\n",
        "    validation_acc = np.amax(result.history['val_acc']) \n",
        "    print('Best validation acc of epoch:', validation_acc)\n",
        "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW-01RE60Tob"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Copy/download the file\n",
        "fid = drive.ListFile({'q':\"title='HyperasOnColab.ipynb'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('HyperasOnColab.ipynb')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRP3fv1yzAQG",
        "outputId": "4f75aff5-c47f-4997-d23e-b3cd5241039d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "best_run, best_model = optim.minimize(model=create_model,\n",
        "                                          data=data,\n",
        "                                          algo=tpe.suggest,\n",
        "                                          max_evals=5,\n",
        "                                          trials=Trials(),\n",
        "                                          notebook_name = 'HyperasOnColab')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>> Imports:\n",
            "#coding=utf-8\n",
            "\n",
            "try:\n",
            "    from google.colab import drive\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import numpy as np\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import re\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import nltk\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from nltk.tokenize import word_tokenize\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from nltk.corpus import stopwords\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from nltk.stem import WordNetLemmatizer\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from gensim.models import KeyedVectors\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.metrics import classification_report, confusion_matrix\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.ensemble import RandomForestClassifier\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.linear_model import LogisticRegression\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.neighbors import KNeighborsClassifier\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.svm import SVC\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from xgboost import XGBClassifier\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.ensemble import AdaBoostClassifier\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from sklearn.model_selection import train_test_split\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pickle\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from keras.models import Model, model_from_json\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import kerastuner as kt\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from numpy import load\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras import Sequential\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding, Dropout\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    import pandas as pd\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow import keras\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from tensorflow.keras import layers\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from kerastuner.tuners import RandomSearch\n",
            "except:\n",
            "    pass\n",
            "\n",
            "try:\n",
            "    from kerastuner import HyperModel\n",
            "except:\n",
            "    pass\n",
            "\n",
            ">>> Hyperas search space:\n",
            "\n",
            "def get_space():\n",
            "    return {\n",
            "        'LSTM': hp.choice[50, 100, 150, 200],\n",
            "        'dropout': hp.uniform('dropout', 0.1, 0.5),\n",
            "    }\n",
            "\n",
            ">>> Data\n",
            "  1: \n",
            "  2: x_train_load = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/x_train.npy')\n",
            "  3: y_train_load = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/y_train.npy')\n",
            "  4: x_test_load = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/x_test.npy')\n",
            "  5: y_test_load = load('/content/drive/MyDrive/Nokia-nlp/BiLSTM/y_test.npy')\n",
            "  6: \n",
            "  7: \n",
            "  8: \n",
            ">>> Resulting replaced keras model:\n",
            "\n",
            "   1: def keras_fmin_fnct(space):\n",
            "   2: \n",
            "   3:     layer = Embedding(input_dim = 185296,output_dim = 100,weights=[embedding_matrix],input_length=100,trainable=False)\n",
            "   4:     model = Sequential()\n",
            "   5:     model.add(layer)\n",
            "   6:     for i in range(3):\n",
            "   7:       model.add(Bidirectional(LSTM(space['LSTM'], dropout= space['dropout'], return_sequences=True)))\n",
            "   8:     # model.add(Bidirectional(LSTM(128, dropout=0.3, return_sequences=True)))\n",
            "   9:     # model.add(Bidirectional(LSTM(100, dropout=0.1, return_sequences=True)))\n",
            "  10:     model.add(Conv1D(64, 5, activation='relu'))\n",
            "  11:     model.add(GlobalMaxPool1D())\n",
            "  12:     model.add(Dense(32, activation='relu'))\n",
            "  13:     model.add(Dense(1, activation='sigmoid'))\n",
            "  14: \n",
            "  15:     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
            "  16: \n",
            "  17:     result = model.fit(x_train, y_train,batch_size=512,epochs=7,validation_split=0.2,callbacks=callbacks,verbose=1) \n",
            "  18: \n",
            "  19:     validation_acc = np.amax(result.history['val_acc']) \n",
            "  20:     print('Best validation acc of epoch:', validation_acc)\n",
            "  21:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
            "  22: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-35135b0a9953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                           \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                           \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                           notebook_name = 'HyperasOnColab')\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperas/optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[0;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[1;32m    132\u001b[0m     return (\n\u001b[1;32m    133\u001b[0m         fmin(keras_fmin_fnct,\n\u001b[0;32m--> 134\u001b[0;31m              \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m              \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m              \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/temp_model.py\u001b[0m in \u001b[0;36mget_space\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
          ]
        }
      ]
    }
  ]
}